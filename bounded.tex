\chapter{Bounded Realisability}
\label{ch:bounded}

\newtheorem*{exmp}{Example}
\newtheorem*{exmpI}{Example: Intuition behind the algorithm}

In this chapter I will describe my work on bounded realisability of reactive systems with safety properties. As introduced in Chapter~\ref{ch:background} reactive realisability is the problem of determining the existence of a program, which we call a \emph{controller}, that continuously interacts with its environment in adherence with a specification. A safety property is a simple condition that defines a set of \emph{error states} that the controller must avoid in order to be correct.

Realisability is the first step on the path to synthesis. In the subsequent chapter I will describe an algorithm that extracts the actions of the controller necessary for realisation. This strategy may be used for synthesis: automatic construction of the controller program. Reactive synthesis for controllers with safety properties has many practical uses in areas such as circuit design, device drivers, or industrial automation.

The algorithm described in this chapter solves bounded safety games. Recall that Chapter~\ref{ch:background} introduced games as a formalism for synthesis by stating the problem in terms of a game between a controller and its environment. In this chapter we are concerned with \emph{bounded} games that restrict all runs in the game to certain length. This concept is borrowed from model checking where it is used to verify that a program emits no erroneous traces of a certain length. A propositional formula may be constructed that is satisfiable when a trace that visits an error state exists. A SAT solver can be used to efficiently search for a satisfying assignment to this formula, which represents a counterexample to the correctness property of the specification. 

In realisability it is not enough to check for the existence of a trace that visits an error state. Such a trace only implies a counterexample that requires the controller to cooperate with the environment to fail. Instead we are interested in strategies for the players. A controller strategy must avoid the error states for all possible environment actions. Likewise, a counterexample strategy must take into account all controller actions. We cannot use a SAT solver to search for a strategy directly as now we require quantifiers. We can, however, check if a strategy allows a counterexample trace without quantification. This sets us up for a counterexample guided methodology in which we construct candidate strategies and check them for correctness. If we discover a counterexample we use it to guide a refinement step in which we improve the candidate strategy.

Similar to bounded model checking, bounded realisability is not a complete procedure.  If we decide that the controller can avoid error states for a game bounded to $k$ rounds there is no guarantee that the environment can not force an error in a game with a bound higher than $k$. In Chapter~\ref{ch:unbounded} I present an extension to the algorithm that extends this algorithm to unbounded games.

Restricting ourselves to solving a bounded safety game enables us to turn the focus of the algorithm from states to traces. The traditional approach of constructing a binary decision diagram to symbolically represent the winning region has the potential to consume exponential space. The advantage of concentrating on runs of the game is that we do not rely on computing the winning states and therefore do not suffer from the related state explosion. The factors affecting the upper limit on scalability for the bounded synthesis algorithm are different to those of the BDD based approach. The most efficient algorithm for a realisability problem depends on the properties of that problem instance.

\section{Algorithm}

This work draws inspiration from a QBF solving algorithm that treats the QBF problem as a game~\cite{Janota12}. In that algorithm one player assumes the role of the universal quantifiers and the opponent takes on the existential quantifiers. In the game, the players take turns to chooses values for their variables from the outermost quantifier block in. Quantifiers may be removed from a formula by iteratively constructing and merging copies of the formula for each quantified variable. The copies of the formula represent the two possible values, true or false, of the quantified boolean variable. Universal quantification can then be reduced to the conjunction of these copies, and existential quantifications corresponds to a disjunction. In practice, these expanded formulas are far too large to be solved so the authors introduce abstractions, or partially expanded formulas, to avoid expanding on variables unnecessarily. The abstractions are refined through a CEGAR process of searching for candidate solutions and analysing counterexamples. The full algorithm is described in detail in Chapter~\ref{ch:relatedwork}. 

We define a safety game by the tuple $G = (S, \mathcal{U}, \mathcal{C}, \delta, s_0, E)$. $S$, $\mathcal{U}$, and $\mathcal{C}$ are sets of boolean variables representing game states, environment actions, and controller actions respectively. The transition relation, $\delta$, is a boolean formula $2^{S} \times 2^{\mathcal{U}} \times 2^{\mathcal{C}} \to 2^{S}$ that maps current states and actions to successor states. The game begins in the initial state $s_0$ and the $E$ is the set of error states that the controller must avoid.

The following quantified formula may be used to solve realisability of a safety game bounded to $k$ game rounds: 
\begin{equation*}
    \begin{multlined}
        \forall u_0 \exists c_0 ... \forall u_k \exists c_k \exists s_{0..k+1} \\
        \shoveright{[\lnot E(s_0) \land \delta(s_0, u_0, c_0, s_1) \land ... \land \lnot E(s_k) \land \delta(s_k, u_k, c_k, s_{k+1}) \land \lnot E(s_{k+1})]}
    \end{multlined}
\end{equation*}

The formula is constructed by unrolling the transition relationship for every game round until the bound is reached. A quantifier alternation is introduced to the formula for the variables corresponding to the actions of each player. Universal quantifiers are used for the environment variables and existential for the controller. The formula is constrained so that if the state at any game round is an error state the formula evaluates to false. In this way the formula is a satisfiable if and only if a strategy exists for the controller that avoids the error states.

It is possible to solve the QBF na\"ively but we can do better by taking into account structural information in the realisability problem that is lost in the translation to prenex normal form. In particular, awareness that the formula is constructed of a repeated transition relation enables more effective learning. A na\"ive solver may learn sets of actions in order to prune the search tree. Instead we learn sets of states and, where possible, we transmit learned information between game rounds. For example, if we discover that the environment can force a visit to an error state from some set of states $s$ in $n$ rounds we also know that the environment can win from $s$ in $n+m$ rounds for all $m > 1$. Knowledge of how to construct the formula also enables the solver to efficiently produce a smaller expanded formula when necessary by simply copying only the parts of the formula corresponding to game rounds after the action of the expanded quantifier.

\subsection{Example}
\label{sec:boundedexample}

We introduce an example to assist an intuitive explanation of the algorithm. Consider a simple model of an ethernet device driver. The operating system makes requests of the driver to write or read data to or from the device. It is the role of the driver to grant these requests while ensuring that a \texttt{read} never occurs when a \texttt{write} was requests and vice versa.

As detailed in Chapter~\ref{ch:background}, we formalise realisability by a game structure $G = (S, \mathcal{U}, \mathcal{C}, \delta, s_0)$. The structure for our example is:

\begin{itemize}
    \item $S = \{ \texttt{request}, \texttt{error} \} $. The game consists of two boolean variables to denote the current request, and whether an error has occurred. We use \texttt{request = 0} to represent a \texttt{read} and \texttt{request = 1} for \texttt{write}.
    \item $\mathcal{U} = \{ \texttt{os\_request} \} $. The uncontrollable actions consists of a single boolean variable to describe a \texttt{read} or a \texttt{write}. We use the same values as before, 0 for \texttt{read} and 1 for \texttt{write}.
    \item $\mathcal{C} = \{ \texttt{dev\_cmd} \}$. The controllable actions similarly consists of a single boolean variable to denote the command given to the device: a \texttt{read} (\texttt{dev\_cmd = 0}) or a \texttt{write} (\texttt{dev\_cmd = 1}).
    \item The transition relation $\delta$ is defined by the following formula:

        $$ \texttt{request'} \gets \texttt{os\_request} $$

        $$ \texttt{err'} \gets \texttt{request} \neq \texttt{dev\_cmd} $$

        Primed variables are used here to indicate how the value is assigned in the next game round.

    \item $s_0 = \{ \texttt{request = 0, err = 0} \}$. To simplify the example there is no idle state so the model is initialised with a pending \texttt{read} request.

\end{itemize}

\tikzset{
    pil/.style={
        -{Latex[length=3mm]},
        thick
    },
    snode/.style={
        align=center,
        circle,
        draw,
        thick
    }
}

\begin{figure}
    \centering
    \begin{tikzpicture}[level distance = 5mm,baseline]
            \node [snode] (n00){(0, 0)};
            \node [snode,right=2cm of n00] (n10){(1, 0)};
            \node [snode,accepting,below=of n00] (n01){(0, 1)};
            \node [snode,accepting,below=of n10] (n11){(1, 1)};

            \draw [pil] (n00) edge [bend left] node [above] {(1, 0)} (n10);
            \draw [pil] (n10) edge [bend left] node [below] {(0, 1)} (n00);

            \draw [pil] (n00) edge [loop left] node [left] {(0, 0)} (n00);
            \draw [pil] (n10) edge [loop right] node [right] {(1, 1)} (n10);

            \draw [pil] (n00) edge node [left] {(*, 1)} (n01);
            \draw [pil] (n10) edge node [right] {(*, 0)} (n11);
    \end{tikzpicture}
    \caption{State automata representation of $\delta$ in Example 1. Nodes are labelled by the tuple (\texttt{request}, \texttt{error}). Edges are labelled with uncontrollable and controllable actions: (\texttt{os\_request}, \texttt{dev\_cmd}). A star indicates that the transition occurs on both a 0 and a 1. Transitions from error states are elided for simplicity.}
    \label{fig:example1}
\end{figure}

The bounded synthesis algorithm is set within a counterexample guided abstraction refinement framework. An abstraction serves a dual purpose in this approach as both a representation of a player strategy and as a way to reduce the search space of the game. This is achieved by employing one player's candidate strategy as its opponent's game abstraction. The effect is that the search for a player's strategy is directed by its opponent's current best effort strategy. Intuitively both players escalate their strategies until one of them converges on a winning strategy.

The abstractions of the game that we construct during the CEGAR search restrict actions available to one of the players.  Specifically, we consider abstractions represented as trees of actions, referred to as \emph{abstract game trees} (AGTs).  Figure~\ref{fig:agt} shows an example abstract game tree restricting the environment (abstract game trees restricting the controller are similar).  In the abstract game, the controller can freely choose actions whilst the environment is required to pick actions from the tree.  After reaching a leaf, the environment continues playing unrestricted.  The tree in Figure~\ref{fig:agt} restricts the first environment action to \texttt{os\_request=1}. At the leaf of the tree the game continues unrestricted.

\captionsetup[subfigure]{singlelinecheck=off,justification=centering}
\tikzset{every node/.style={solid}}
\tikzset{align at top/.style={baseline=(current bounding box.north)}}
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw,inner sep=1pt] (root){}
                child {node [circle,draw,inner sep=1pt] {}
                    child {node [circle,draw,inner sep=1pt] {}
                        child {node [circle,draw,inner sep=1pt] {}
                            node [left=4pt] {\texttt{cmd=0}}
                        }
                        node [left=4pt] {\texttt{req=1}}
                    }
                node [left=4pt] {\texttt{cmd=1}}
                }
                node [left=4pt] {\texttt{req=1}}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Trace to error}
        \label{fig:example1a}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    edge from parent node [left] {\texttt{req=1}}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Candidate environment strategy}
        \label{fig:example1b}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw,inner sep=1pt] (root){}
                child {node [circle,draw,inner sep=1pt] {}
                    child {node [circle,draw,inner sep=1pt] {}
                        child {node [circle,draw,inner sep=1pt] {}
                            node [left=4pt] {\texttt{cmd=1}}
                        }
                        node [left=4pt] {\texttt{req=1}}
                    }
                node [left=4pt] {\texttt{cmd=0}}
                }
                node [left=4pt] {\texttt{req=1}}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Counterexample trace}
        \label{fig:example1c}
    \end{subfigure}
    \caption{Execution of bounded realisability on the example.}
    \label{fig:example1exe}
\end{figure}

We will now step through an execution of the algorithm using the example just introduced. The first step involves a search of the empty game abstraction for an initial candidate strategy for the environment player. In the empty abstraction we have not yet restricted the game in any way so all runs through the game are enabled. We search for a candidate strategy by finding a run that reaches an error state. Here we are only searching for the existence of a run and so we do not require quantifier alternations and a SAT solver can be used to efficiently perform the search. Intuitively, an existential search is equivalent to the two players of the game cooperating. In most real world applications of synthesis the environment is in fact cooperating with the system so the cooperation heuristic turns out to be practical. Figure~\ref{fig:example1a} shows a trace through the example game that reaches an error state.

The trace informs us that by playing the actions contained in the trace it is possible for the environment to reach an error state. From this we conjecture that the first action in the trace is a reasonable choice for the environment's winning strategy. So we construct a candidate strategy in which the environment plays \texttt{os\_request=1} in the first game round. The next step is to validate our conjecture by searching for counterexamples. We do this by constructing a new abstraction of the game in which the environment is restricted to playing actions from its candidate strategy (Figure~\ref{fig:example1b}). Then we play this abstract game on the behalf of the controller. Once again we search for a trace through the game but this time the SAT solver is searching for a trace that avoids error states for the duration of the bounded game. Any traces found in this way indicate the possibility of a spoiling strategy for the controller that defeats the candidate strategy of the environment. Figure~\ref{fig:example1c} shows a trace in which the controller counters the environment by playing \texttt{dev\_cmd=0} in the first round to match the initial state of \texttt{request=0}.

The trace can be used to label nodes in the AGT representing a partial strategy for the controller for that abstract game (Figure~\ref{fig:example1d}). Our goal now is to refine the candidate strategy for the environment so that it wins against the controller's partial strategy. In the candidate strategy we have not yet selected an environment action for the second game round, so we may refine the strategy by doing this now. Since the game is deterministic we can capture the actions in the AGT and the counterexample strategy with a single state: $\{ \texttt{request=1, err=0} \}$. We now solve an unrestricted abstract game with a bound of 1 from this state in order to determine which action the environment should select for the second game round. 

The shorter game is solve via a recursive call to the algorithm. First a trace to an error state is found (Figure~\ref{fig:example1e}) and a candidate constructed followed by a counterexample trace in which the controller chooses to correctly play \texttt{cmd=1}. At this point it is impossible to refine the environment candidate strategy by appending additional actions because the bound on game length has been reached. Instead an action from the candidate is backtracked and the search continues on a refined AGT that now includes the counterexample (Figure~\ref{fig:example1f}).


\begin{figure}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    node [left=4pt] {\texttt{cmd=0}}
                    edge from parent node [left] {\texttt{req=1}}
                }
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Partial strategy}
        \label{fig:example1d}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw,inner sep=1pt] (root){}
                child {node [circle,draw,inner sep=1pt] {}
                    node [left=4pt] {\texttt{cmd=0}}
                }
                node [left=4pt] {\texttt{req=1}}
                node [above=4pt] {$\langle (1, 0), 1 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Trace to error}
        \label{fig:example1e}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    edge from parent node [left] {\texttt{cmd=1}}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (1, 0), 1 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Refined AGT}
        \label{fig:example1f}
    \end{subfigure}%
    \caption{Continued example algorithm execution}
    \label{fig:example1execont}
\end{figure}

The environment is now playing against a restricted opponent. In this case there is no possible action that the controller can take to reach an error state. This can be seen in the state machine for the game in Figure~\ref{fig:example1}. If the environment cannot win from this state against a restricted controller then clearly it cannot win against an unrestricted controller in the game bounded to one round. We can therefore conclude that the candidate strategy that led to this state was not a winning strategy for its abstract game.

Actually we may conclude a stronger assertion that any strategy that results in this state with one game round remaining is a bad strategy. It is possible to exclude these strategies from future searches in an optimisation described in Section~\ref{sec:boundedLearning}.

The algorithm now backtracks to the very beginning. We have determined that the candidate environment strategy in Figure~\ref{fig:example1b} can be defeated by the partial controller strategy in Figure~\ref{fig:example1d}. Now the abstraction of the game is refined to include the counterexample. The original empty abstraction refined with a single counterexample action is shown in Figure~\ref{fig:example1g}. Note that a trace reaching an error in which the environment plays \texttt{os\_request=1} is still possible in this abstract game. The algorithm without optimisation will consider this candidate again for the new abstract game. The result will be the same and refinement occurs again, except now the game is refined to include a counterexample action from the second round (Figure~\ref{fig:exampl1h}). On this game abstraction the candidate is blocked because the trace must include \texttt{dev\_cmd=1} in the second round.

As a result the environment must choose a different action for the first round of the game. A SAT query will reveal that \texttt{os\_request=0} can lead to an error when the controller plays \texttt{dev\_cmd=1} in the second round. However, this candidate can be defeated by the controller choosing \texttt{dev\_cmd=0} instead. The algorithm will discover this and refine the abstraction again to include the counterexample. In refined abstract game (Figure~\ref{fig:example1i}) the environment has no winning trace and therefore the algorithm terminates and returns realisable for the controller.

\begin{figure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    edge from parent node [left] {\texttt{cmd=0}}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Refined AGT}
        \label{fig:example1g}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{cmd=1}}
                    }
                    edge from parent node [left] {\texttt{cmd=0}}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Refined AGT}
        \label{fig:example1h}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{cmd=1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{cmd=0}}
                    }
                    edge from parent node [left] {\texttt{cmd=0}}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Refined AGT}
        \label{fig:example1i}
    \end{subfigure}%
    \caption{Continued example algorithm execution}
    \label{fig:example1execont2}
\end{figure}


\subsection{Abstract game trees}

An abstract game tree (AGT) is a restricted version of the concrete game in which fewer actions are available. The root of the tree is annotated by the initial state $s$ of the abstract game and the bound $k$ on the number of rounds.  We denote $\textsc{nodes}(T)$ the set of all nodes of a tree $T$, $\textsc{leaves}(T)$ the subset of leaf nodes.  For edge $e$, $\textsc{action}(e)$ is the action that labels the edge, and for node $n$, $\textsc{height}(k, n)$ is the distance from n to the last round of a game bounded to $k$ rounds.  $\textsc{height}(k, T)$ is the height of the root node of the tree.  For node $n$ of the tree, $\textsc{succ}(n)$ is the set of pairs $\langle e, n' \rangle$ where $n'$ is a child node of $n$ and $e$ is the edge connecting $n$ and $n'$.

Given an environment (controller) abstract game tree $T$ a \emph{partial strategy} $Strat: \textsc{nodes}(T) \rightarrow \mathcal{C}$ ($Strat: \textsc{nodes}(T) \rightarrow \mathcal{U}$) labels each node $n$ of the tree with the controller's (environment's) action to be played in the game round corresponding to $\textsc{height}(k, n)$.  Given a partial strategy $Strat$, we can map each leaf $l$ of the abstract game tree to $\langle s',i'\rangle=\textsc{outcome}(\langle s, i\rangle, Strat, l)$ obtained by playing all controllable and uncontrollable actions on the path from the root to the leaf.  An environment (controller) partial strategy is \emph{winning against $T$} if all its outcomes are states that are winning for the environment (controller) in the concrete game.

\subsection{Counterexample guided realisability}

The bounded realisability algorithm constructs candidate strategies for one player that serve the dual purpose of game abstraction for its opponent. The algorithm begins by discovering a candidate for the environment. Next we must determine is there are counterexamples to the candidate. This step is executed by constructing an abstract game tree from the environment's candidate strategy and recursively invoking the algorithm on this new abstraction. The recursive call plays against the environment's strategy on behalf of the controller. Thus the algorithm can be seen as running two competing solvers, for the controller and for the environment. By symmetrically playing for both players achieve the goal of directing the search towards strong strategies and counterexamples.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{solveAbstract}{$p, s, k, T$}
        \State $cand \gets $ \Call{findCandidate}{$p, s, k, T$} \Comment{Look for a candidate}
        \IIf{$k = 1$} \Return $cand$ \EndIIf \Comment{Reached the bound}
        \State $T' \gets T$
        \Loop
            \If{$cand = \texttt{NULL}$} \Comment{No candidate: return with no solution}
                \State \Return $\texttt{NULL}$
            \EndIf 
            \State $\langle cex, l, u \rangle \gets $ \Call{verify}{$p, s, k, T, cand$} \Comment{Verify candidate}
            \If{$cex = \False$} \Comment{No counterexample: return candidate}
                \State \Return $cand$ 
            \EndIf 
            \State $T' \gets $ \Call{append}{$T', l, u$} \Comment{Refine $T'$ with counterexample}
            \State $cand \gets $ \Call{solveAbstract}{$p, s, k, T'$} \Comment{Solve refined game tree}
        \EndLoop
        \EndFunction
        \algstore{b1}
    \end{algorithmic}
    \caption{Solve an abstract bounded game}
    \label{alg:solveabstract}
\end{algorithm}


The full procedure is illustrated in \Cref{alg:solveabstract,alg:findcandidate,alg:verify}. \textsc{solveAbstract} takes a concrete game $G$ with maximum bound $\kappa$ as an implicit argument.  In addition, it takes a player $p$ (controller or environment), state $s$, bound $k$ and an abstract game tree $T$ and returns a winning partial strategy for $p$, if one exists.  The initial invocation of the algorithm takes the initial state $I$, bound $\kappa$ and an empty abstract game tree $\emptyset$.  Initially the solver is playing on behalf of the environment since that player takes the first move in every game round.  The empty game tree does not constrain opponent moves, hence solving such an abstraction is equivalent to solving the original concrete game.  

The algorithm is organised as a counterexample-guided abstraction refinement (CEGAR) loop.  The first step of the algorithm uses the \textsc{findCandidate} function, described below, to come up with a candidate partial strategy that is winning when the opponent is restricted to $T$.  If it fails to find a strategy, this means that no winning partial strategy exists against the opponent playing according to $T$.  If, on the other hand, a candidate partial strategy is found, we need to verify if it is indeed winning for the abstract game $T$.

The \textsc{verify} procedure searches for a \emph{spoiling} counterexample strategy in each leaf of the candidate partial strategy by calling \textsc{solveAbstract} for the opponent. The dual solver solves games on behalf of the opponent player.  

\begin{algorithm}
    \begin{algorithmic}
        \algrestore{b1}
        \Function{findCandidate}{$p, s, k, T$}
        \State $\hat{T} \gets $ \Call{extend}{$T$} \Comment{Extend the tree with unfixed actions}
            \If{$p = \texttt{cont}$}
                \State $f \gets $ \Call{treeFormula}{$k, \hat{T}$}
            \Else
                \State $f \gets $ \Call{\textoverline{treeFormula}}{$k, \hat{T}$}
            \EndIf
            \State $sol \gets $ \Call{SAT}{$s(X_{\hat{T}}) \land f$}
            \If{$sol = \texttt{unsat}$} 
                \State \Return $\texttt{NULL}$ \Comment{No candidate exists}
            \Else
                \LineComment{Return partial strategy for $T$}
                \State \Return $\{ \langle n, c \rangle\ |\ n \in $ \Call{nodes}{$T$}$, c = \Call{sol}{n} \}$
            \EndIf
        \EndFunction
        \algstore{b2}
    \end{algorithmic}
    \caption{Find a candidate strategy}
    \label{alg:findcandidate}
\end{algorithm}


\begin{algorithm}
    \begin{algorithmic}
        \algrestore{b2}
        \Function{verify}{$p, s, k, T, cand$}
            \For{$l \in leaves(gt)$}
            \State $\langle k', s'\rangle \gets $ \Call{outcome}{$s, k, cand, l$} \Comment{Get bound and state at leaf}
            \If{$p = \textsc{cont}$}
                \State $T' \gets \emptyset$
            \Else
                \State $T' \gets \{ cand(l) \}$
            \EndIf
                \LineComment{Solve for the opponent}
                \State $a \gets $ \Call{solveAbstract}{\Call{opponent}{$p$}, $s'$, $k'$, $T'$} 
                \IIf{$a \neq \texttt{NULL}$} \Return $\langle \True, l, a \rangle$ \EndIIf \Comment{Return counterexample}
            \EndFor
            \State \Return $\langle \False, \emptyset, \emptyset \rangle$ \Comment{There was no counterexample}
        \EndFunction
    \end{algorithmic}

    \caption{Verify a candidate strategy}
    \label{alg:verify}
\end{algorithm}

If the dual solver can find no spoiling strategy at any of the leaves, then the candidate partial strategy is a winning one. Otherwise, \textsc{verify} returns the move used by the opponent to defeat a leaf of the partial strategy, which is appended to the corresponding node in $T$ in order to refine it in line~(9).  

We solve the refined game by recursively invoking \textsc{solveAbstract} on it.  If no partial winning strategy is found for the refined game then there is also no partial winning strategy for the original abstract game, and the algorithm returns a failure.  Otherwise, the partial strategy for the refined game is \emph{projected} on the original abstract game by removing the leaves introduced by refinements. The resulting partial strategy becomes a candidate strategy to be verified at the next iteration of the loop. In the worst case the loop terminates after all actions in the game are refined into the abstract game.  

The CEGAR loop depends on the ability to guess candidate partial strategies in \textsc{findCandidate}. For this purpose we use the heuristic that a partial strategy may be winning if each \textsc{outcome} of the strategy can be extended to a run of the game that is winning for the current player.  Clearly, if such a partial strategy does not exist then no winning partial strategy can exist for the abstract game tree. We can formulate this heuristic as a SAT query, which is constructed recursively by $\textsc{treeFormula}$ (for the controller) or $\textsc{\textoverline{treeFormula}}$ (for the environment) in Algorithm~\ref{alg:treeFormula}.  

The tree is first extended to the maximum bound with edges that are labeled with arbitrary opponent actions (Algorithm~\ref{alg:bounded}, line 14).  For each node in the tree, new SAT variables are introduced corresponding to the state ($X_T$) and action ($U_T$ or $C_T$) variables of that node. Additional variables for the opponent actions in the edges of $T$ are introduced ($U_e$ or $C_e$) and set to $\textsc{action}(e)$.  The state and action variables of node $n$ are connected to successor nodes $\textsc{succ}(n)$ by an encoding of the transition relation and constrained to the winning condition of the player.  

%%%Then copies of state and action variables are introduced for each node in the
%%%tree and opponent action variables for each edge $e$ are set to
%%%$\textsc{action}(e)$. 

%%%Candidates are discovered by passing a formulation of the abstract game tree to
%%%a SAT solver in $\textsc{findCandidate}$. This formula contains CNF encodings
%%%of all of the unrolled runs represented by the tree and the winning condition
%%%of the current player.  Runs are encoded by copying the transition relation for
%%%every step in the abstract game. When playing for the controller, the SAT
%%%solver searches for a satisfying assignment to the unfixed label variables in
%%%tree so that none of the runs reaches the error state. The environment
%%%formulation is satisfiable if any run does reach the error state.  The formula
%%%is constructed recursively from the root of a tree by $\textsc{treeFormula}$
%%%(see Algorithm~\ref{alg:treeFormula}).

%%%Since the game tree formulation is passed to a SAT solver, both controllable
%%%and uncontrollable unfixed labels will be existentially quantified. This means
%%%that the SAT solver will find any way to win the game while both players are
%%%cooperating. If no winning run exists in an abstract game even when the players
%%%are cooperating then there is no winning run when the opponent is playing
%%%adversarily. When a winning run is found, the actions chosen by the SAT solver
%%%are used to refine the game tree. This is advantageous for many synthesis
%%%problems where the game must be formalised as adversarial for correctness but
%%%the final implementation will cooperate with its environment in the real world.
%%%An example of such a system is a device driver that cooperates with the device
%%%and OS to provide the interface between the two.

\begin{algorithm}
    \caption{Tree formulas for Controller and Environment}
    \label{alg:treeFormula}
    \begin{algorithmic}[1]
        \Function{treeFormula}{$k, T$}
        \If{$\Call{height}{k, T} = 0$}
        \State \Return{ $\lnot \Call{E}{X_{T}}$ }
        \Else
        \State \Return{$\lnot \Call{E}{X_{T}} \land$ \\
            $$\bigwedge_{\langle e, n \rangle \in \Call{succ}{T}}(\Call{$\delta$}{X_T, U_e, C_T, X_n} \land U_e = \Call{action}{e} \land \Call{treeFormula}{k, n})$$
        }
        \EndIf
        \EndFunction
        \algstore{tf1}
    \end{algorithmic}

    \begin{algorithmic}[1]
        \algrestore{tf1}
        \Function{\textoverline{treeFormula}}{$k, T$}
        \If{$\Call{height}{k, T} = 0$}
        \State \Return{\Call{E}{$X_{T}$}}
        \Else
        \State \Return{ $\Call{E}{X_{T}} \lor$ \\
        $$\bigvee_{\langle e, n \rangle \in \Call{succ}{T}}(\Call{$\delta$}{X_T, U_T, C_e, X_n} \land C_e = \Call{action}{e} \land \Call{\textoverline{treeFormula}}{k, n})$$ }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Correctness}

Completeness of the algorithm follows from the completeness of the backtracking search. In the worst case the algorithm will construct the entire concrete game tree and effectively expand all quantifiers. Soundness follows from the existential search of the SAT solver in \textsc{findCandidate}. The algorithm terminates after searching for a candidate strategy on an abstract game tree with actions fixed only for the opponent. If no candidate can be found with the opponent restricted in this way then no strategy exists for the player.

\section{Optimisations}

The bounded realisability algorithm has an exponential worst case running time when the entire search tree must be explored before discovering a winning strategy. In this section I present some optimisations that aim to prune the search tree as well as discover winning strategies earlier in the search.

\subsection{Bad State Learning}
\label{sec:boundedLearning}

The most important optimisation that allows the algorithm to avoid much of the search space is to record states that are known to be losing for one player. On subsequent calls to the SAT solver we encode these states in the candidate strategy formula (see Algorithm~\ref{alg:treeFormulaLearning}). Thus the algorithm avoids choosing moves that lead to states that are already known to be losing.

Bad states are learned from failed attempts to find a candidate. If the SAT solver cannot find a candidate strategy for a given abstract game tree that means that there is a fixed prefix in the game tree for which the current player can never win. The state reached by playing the moves in the prefix must then be a losing state with some caveats. If the state is at the node with height $k$ and losing for the environment then we know that the environment cannot force to the error set in $k$ rounds. We do not know if the environment can force to the error set in $> k$ rounds. Therefore we record losing states for the environment in an array of sets of states $B^e$ indexed by the height at which the set is losing. For the controller, a losing state is losing for any run of length $>= k$. In practical use we are uninterested in controller strategies that make use of states that would lose should the game be extended to a longer bound so we merely maintain a single set of controller losing states $B^c$.

Additional states can be learned by expanding a single state into a set of losing states by greedily testing each variable of the state for inclusion in a \emph{cube} of states. This technique is well known in the literature and can be efficiently implemented using a SAT solver capable of solving under assumptions~\cite{Een03}. It is shown in Algorithm~\ref{alg:boundedLearning}.

\begin{algorithm}
    \caption{Modified Tree Formulas with Bad State Avoidance}
    \label{alg:treeFormulaLearning}
    \begin{algorithmic}[1]
        \Function{treeFormula}{$k, T$}
        \If{$\Call{height}{k, T} = 0$}
        \State \Return{ $\lnot \Call{$B^c$}{X_{T}}$ }
        \Else
        \State \Return{$\lnot \Call{$B^c$}{X_{T}} \land$ \\
            $$\bigwedge_{\langle e, n \rangle \in \Call{succ}{T}}(\Call{$\delta$}{X_T, U_e, C_T, X_n} \land U_e = \Call{action}{e} \land \Call{treeFormula}{k, n})$$
        }
        \EndIf
        \EndFunction
        \algstore{tf1}
    \end{algorithmic}

    \begin{algorithmic}[1]
        \algrestore{tf1}
        \Function{\textoverline{treeFormula}}{$k, T$}
        \If{$\Call{height}{k, T} = 0$}
        \State \Return{\Call{E}{$X_{T}$}}
        \Else
        \State \Return{\Call{$B^e$[\Call{height}{k,t}]}{$X_{T}$} $\lor$ \\
        $$\bigvee_{\langle e, n \rangle \in \Call{succ}{T}}(\Call{$\delta$}{X_T, U_T, C_e, X_n} \land C_e = \Call{action}{e} \land \Call{\textoverline{treeFormula}}{k, n})$$ }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Strategy Shortening}

Learning new bad states means reducing the search space for the algorithm. It follows that it is better to learn states earlier in the algorithm's execution. One problem with relying on SAT calls that assume cooperation is that there is no urgency to the returned candidate strategies. Consider the running example: the environment can reach the error set by setting \texttt{request} to 2 during two rounds. However, in the empty abstract game tree of a bounded game of length 3 or longer, there is no reason for the SAT solver to make the first action one of the requesting rounds if it can assume the environment will never grant any resources. The first action is important because the candidate strategy is derived from that. The candidate is what the opponent has the chance to respond to, so if the candidate does not do anything useful the opponent's response has the freedom to be equally apathetic about reaching its goal. This leads to much of the search space being explored unnecessarily until we learn a losing state.

Encouraging the SAT solver to find \emph{shorter} strategies is a successful heuristic for mitigating this issue. Whilst it does require more SAT calls per call to \textsc{findCandidate} it can be efficiently implemented using incremental SAT solving and during our benchmarking we found the cost to be worthwhile. A strategy is shorter if following the strategy leads to a known bad state for the opponent is fewer game rounds. For the environment this is clearly analogous to reaching the error set sooner. For the controller it is less clear but intuitively states that are environment losing at a certain height are more likely to be \emph{safe} states from which the controller may be able to force a loop.

\begin{algorithm}
    \caption{Strategy Shortening}
    \label{alg:strategyShortening}
    \begin{algorithmic}[1]
        \Function{shorten}{$p, s, k, T$}
            \State $\hat{T} \gets $ \Call{extend}{$T$}
            \State $f \gets $ \IfElse{$p = \texttt{cont}$}{\Call{treeFormula}{$k, \hat{T}$}}{\Call{\textoverline{treeFormula}}{$k, \hat{T}$}} \EndIfElse
            \State $prev \gets \top$
            \For{$l \in leaves(gt)$}
                \State $n \gets $ \Call{root}{$l$}
                \While{\Call{height}{$k, n$} $\neq 0$}
                    \If{$p = \texttt{cont}$}
                        \State $a \gets $ \Call{$B^e$[\Call{height}{$k, l$}]}{$X_n$}
                    \Else
                        \State $a \gets $ \Call{$E$}{$X_n$}
                    \EndIf
                    \State $sol \gets $ \Call{SATWithAssumptions}{$prev \land a$, $s(X_{\hat{T}}) \land f$}
                    \If{$sol \neq \texttt{NULL}$}
                        \State $prev \gets prev \land a$
                        \Break
                    \EndIf
                        
                    \State $n \gets $ \Call{succ}{$n$}
                \EndWhile
            \EndFor
            \State \Return $sol$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Default Actions}

During the search for a candidate strategy the SAT solver selects actions for the opponent as though the players are cooperating. Sometimes the result is an action that will always fail for the opponent. In many specifications the environment is given the option to fail as a way of modelling errors. For example, in a network driver specification error transitions may be used to model failed connections. When such a transition exists it will often be selected by the SAT solver (especially when the strategy shortening optimisation is enabled). Constantly selecting a bad action for the opponent significantly affects the performance of the algorithm because no bad states can be learned and the solver must refine the game abstraction to avoid the bad action. Additionally, if a candidate strategy was found by relying on a bad action then it will usually need to be backtracked. 

To avoid problematic action selection the solver can instead use some heuristic to select the arbitrary action required in the SAT call in \textsc{findCandidate}. This does not affect the correctness of the algorithm. If no candidate can be found with the opponent playing an arbitrary action then clearly the selected action (or a different opponent action that is winning) would have eventually been refined into the abstract game if the opponent instead cooperated. A simple action selection heuristic has been observed to improve the performance of the solver during benchmarking. Before the main algorithm executes two SAT calls are made with formulas constructed from \textsc{treeFormula} and \textsc{\textoverline{treeFormula}} called on an empty abstract game tree. From the result a mapping of height to \emph{default action} is made for each player. During \textsc{findCandidate} calls the arbitrary opponent actions are taken from the corresponding map at the appropriate height.


\section{Discussion}

The design of the algorithm is motivated by the desire to solve bounded safety games whilst avoiding the potential state explosion of computing the winning set. The key insight is to shift the emphasis from finding a winning set to finding winning strategies. The shift is made possible by searching for runs in an abstraction of the game and using the results to refine the abstraction. The advantage of this approach is that even when the winning set is difficult to represent symbolically (via a BDD or similar) a winning strategy may still be found. The reverse is also true: if the winning strategy requires too much branching it will become intractable to construct it using this algorithm. The difference can be likened to breadth-first versus depth-first search, the appropriate algorithm depends on the particular problem instance to be solved.

\subsection{Comparison to QBF}

As previously stated the bounded realisability problem is a specialisation of QBF and the algorithm presented in this chapter is a domain specific QBF solver. In Chapter~\ref{ch:relatedwork} several approaches to solving general QBF problems were described. The state of the art in QBF is focused on several areas: dependency analysis, circuit analysis, and counterexample guided search. 

Analysing dependencies or the original circuit of the presented QBF are both aimed at reconstructing information that was lost during the conversion to a prenex normal form formula. The aim is to use that information to speed up the search for a solution. The bounded realisability algorithm is aware of the original problem formulation and constructs parts of it on the fly. For example, the quantifier dependency graph for a bounded realisability problem would normally be mostly linear due to the construction via unrollings of the transition relation, which rarely contains fully independent subformulas. An exception to this is the set of state and Tseitin variables which belongs to the last existential quantifier in the prefix. Those variables are decided by the action variables preceding them in the game. The algorithm presented here never includes anything other than action variables in player strategies, which reflects an awareness of that dependency. 

The algorithm contains a higher level of information about the original problem than an analysis of the circuit could provide. It may be possible to implement some the propagation techniques introduced by these methods but in general learning sets of states provides a greater pruning of the search tree. We are able to transmit learned states between game rounds, which would not be possible in a general QBF solver.

The ability to learn states is also the primary difference between the specialised solver and the counterexample guided QBF algorithm proposed in \cite{Janota12}. More recent QBF approaches use counterexamples to guide an abstraction of the problem via clause selection. It would interesting to apply these techniques to bounded realisability although the extension to unbounded synthesis in Chapter~\ref{ch:unbounded} would not be applicable without and abstract game tree.

\subsection{Model checking}

The concept of solving games by searching for counterexamples of a certain length with a SAT solver was first introduced in a bounded approach to model checking~\cite{Biere99}. Replacing counterexample traces with trees is the natural extension of this approach to realisability given that the controller now has agency to select actions in the game rather than modelling an implementation. The move from model checking to realisability brings additional complexity to the problem but the efficiency of SAT is still exploited to discover counterexamples quickly.

\subsection{Related synthesis techniques}

Bounded synthesis~\cite{Finkbeiner13} uses an SMT solver to search for a bounded \emph{implementation} for an LTL specification. Lazy synthesis~\cite{Finkbeiner12} similarly searches for a bounded partial implementation and uses BDD based model checking to search for counterexamples in order to refine the partial implementation. Both of these techniques are for full LTL synthesis, which is a different problem to the safety specifications solved in this chapter, but the overarching framework is similar. 

Algorithms that avoid BDDs in favour of SAT solving have been proposed for safety synthesis in the past~\cite{Morgenstern13,Chiang15,Bloem14} but none of these take the approach of unrolling the transition relation to a bound. In these previous works states belonging to a winning region for one player are collected over a series of SAT queries concerning single transitions. These works have more in common with the extension of bounded realisability to unbounded games in Chapter~\ref{ch:unbounded}.

\subsection{Limitations}

The performance of bounded realisability is influenced primarily on the branching factor of the game tree. The worst case scenario occurs when each environment action must be matched with a different controller action and no state learning is possible. For example, consider a modified version of the example given in Section~\ref{sec:boundedexample} in which the environment requests are not latched into a state variable.

\begin{itemize}
    \item $S = \{ \texttt{error} \} $. The only state is whether or not an error has occurred. 
    \item $\mathcal{U} = \{ \texttt{os\_request} \} $. 
    \item $\mathcal{C} = \{ \texttt{dev\_cmd} \}$.
    \item The transition relation $\delta$ is now: $$ \texttt{err'} \gets \texttt{os\_request} \neq \texttt{dev\_cmd} $$
    \item $s_0 = \{ \texttt{err = 0} \}$. 
\end{itemize}

Without a state variable to learn the algorithm is forced to explore all possible actions to determine realisability. If the example is modified again to allow for more types of requests by increasing the size of the action variables then it is easy to see the potential blow up. In Figure~\ref{fig:limitationexample} the final AGT for this example with a bound of $3$ and action variables of size $3$ is shown. The figure shows the values of \texttt{os\_request} in all possible paths through the game. It is clear that this algorithm cannot scale with these kinds of specifications. It should be noted that it is trivial to solve this example with a BDD solver, which can immediately prove that \texttt{err = 0} is a winning region for the controller. The extension to unbounded realisability in Chapter~\ref{ch:unbounded} will also be able to handle this example by constructing the winning region.

\begin{figure}
    \centering
    \begin{tikzpicture}[
        align at top,
        level distance = 10mm,
        level 1/.style={sibling distance=45mm},
        level 2/.style={sibling distance=15mm},
        level 3/.style={sibling distance=5mm}]
        \node [circle,draw] (root){}
            child {node [circle,draw] {}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [left=2mm] {\texttt{0}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right] {\texttt{1}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right=2mm] {\texttt{2}}
                }
                edge from parent node [above left] {\texttt{0}}
            }
            child {node [circle,draw] {}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [left=2mm] {\texttt{0}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right] {\texttt{1}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right=2mm] {\texttt{2}}
                }
                edge from parent node [right] {\texttt{1}}
            }
            child {node [circle,draw] {}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [left=2mm] {\texttt{0}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right] {\texttt{1}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right=2mm] {\texttt{2}}
                }
                edge from parent node [above right] {\texttt{2}}
            }
            node [left=4pt] {}
            node [above=4pt] {$\langle 0, 3 \rangle$};
    \end{tikzpicture}
    \caption{AGT with large branching factor}
    \label{fig:limitationexample}
\end{figure}

\subsection{Strengths}

Bounded realisability is most useful in the case where the winning region of a game has a large BDD but the winning strategy for the game is compact. It is not fair to compare the incomplete bounded algorithm to a complete winning region computation but we may consider the ability for each technique to find counterexamples. 

To demonstrate the usefulness of the algorithm we introduce a simple warehouse robot controller. In this example the warehouse consists of four loading bays and the robot is tasked with shipping items placed in the bays in a timely fashion. We model the problem with a \texttt{timer} variable that begins at one, ticks down to zero, and then resets back to one. At the beginning of the cycle the environment may items in any two bays. The robot then may ship all items in one bay per timer tick and must clear all bays before the timer resets. The example is trivial but it could be scaled on the number of bays, number of items that the environment can load, and the length of the timer to produce complex specifications. In this example we use integer values for \texttt{timer}, \texttt{ship}, \texttt{load0} and \texttt{load1} to make the description more concise.

\begin{itemize}
    \item $S = \{ \texttt{error, bay0, bay1, bay2, bay3, timer} \} $.
    \item $\mathcal{U} = \{ \texttt{load0, load1} \} $. 
    \item $\mathcal{C} = \{ \texttt{ship} \}$.
    \item The transition relation $\delta$ is now: 
        \begin{align*}
            \texttt{error'} \gets&\ \texttt{timer=1}\ \land (\texttt{bay0=1} \lor \texttt{bay1=1} \lor \texttt{bay2=1} \lor \texttt{bay3=1} ) \\
            \texttt{bay0'} \gets&\ (\texttt{timer=1} \land (\texttt{load0=0} \lor \texttt{load1=0})) \lor (\texttt{timer}\neq\texttt{1} \land \texttt{ship=0}) \\
            \texttt{bay1'} \gets&\ (\texttt{timer=1} \land (\texttt{load0=1} \lor \texttt{load1=1})) \lor (\texttt{timer}\neq\texttt{1} \land \texttt{ship=1}) \\
            \texttt{bay2'} \gets&\ (\texttt{timer=1} \land (\texttt{load0=2} \lor \texttt{load1=2})) \lor (\texttt{timer}\neq\texttt{1} \land \texttt{ship=2}) \\
            \texttt{bay3'} \gets&\ (\texttt{timer=1} \land (\texttt{load0=3} \lor \texttt{load1=3})) \lor (\texttt{timer}\neq\texttt{1} \land \texttt{ship=3}) \\
            \texttt{timer'} \gets&\ (\texttt{timer=0})\ \texttt{?}\ \texttt{1}\ \texttt{:}\ (\texttt{timer - 1})
        \end{align*}
    \item $\begin{aligned}[t]
            s_0 = \{& \texttt{error = 0, bay0 = 0, bay1 = 0, bay2 = 0,} \\
                    &\texttt{bay3 = 0, timer = 1} \}
        \end{aligned}$
\end{itemize}

The specification is clearly unrealisable given that in every cycle the environment can load items into two bays and the controller can only remove items from one bay. A realisability solver that uses BDDs might attempt to compute a winning region for the environment. The entire environment winning region for this game contains all possible configurations in which the environment loads two distinct bays. As shown in Figure~\ref{fig:strengthsexamplebdd}, BDDs are not succinct on are in this enumeration of cubes style. If this problem was to scale the BDD would quickly consume a large amount of space.

\begin{figure}
    \centering
    \begin{tikzpicture}[level distance = 20mm,baseline]
        \node [circle,draw] (bay0){\texttt{bay0}}
            child {node [circle,draw] (bay1a) {\texttt{bay1}}
                child {node [circle,draw] (bay2a) {\texttt{bay2}}
                    child {node [circle,draw] (bay3a) {\texttt{bay3}}
                        child {node [draw] (T) {\texttt{T}}}
                    }
                }
                child {node [circle,draw] (bay2a) {\texttt{bay2}}
                }
            }
            child {node [circle,draw] (bay1b) {\texttt{bay1}}
                child {node [circle,draw,right] (bay2b) {\texttt{bay2}}
                    child {node [circle,draw] (bay3b) {\texttt{bay3}}
                        child {node [draw] (F) {\texttt{F}}}
                    }
                }
            }
        ;
        \draw [->] (bay3a) -- (F);
        \draw [->] (bay3b) -- (T);
    \end{tikzpicture}
    \caption{Environment winning region as a BDD}
    \label{fig:strengthsexamplebdd}
\end{figure}

By instead using a SAT solver to check for the existence of a spoiling strategy to the environment filling two bays, the bounded realisability algorithm avoids computing the set of all environment winning states. The difference between the two methodologies is similar to the difference between breadth first search and depth first search. The BDD driven approach explores the game tree level by level similar to BFS and builds a compact set of winning states. The SAT based algorithm present here finds single paths through the tree that may be winning and checks them. Both approaches are efficient on different classes of specifications.

\section{Summary}

In this chapter I presented the fundamental building block of this thesis, a new algorithm for solving bounded realisability. In later chapters I will explain extensions to this algorithm to increase its applicability and in Chapter~\ref{ch:evaluation} I will present results and an evaluation of the contribution of this work.

\begin{itemize}
    \item Here we introduce an algorithm for solving synthesis games that are bounded to a fixed number of game rounds. The algorithm is a counterexample guided abstraction refinement framework in which abstractions of the game are constructed from candidate strategies for the players. This is done in a way that allows a candidate strategy to be checked for a spoiling strategy by playing the game abstraction on behalf of the opponent. Spoiling strategies are counterexamples to a strategy that may be used for refinement.

    \item The design of the algorithm is inspired by the exponential blow up that can result from constructing a symbolic representation of the winning region as a BDD. In this algorithm the winning region is never computed although some winning states are learned as an optimisation to prune the search tree. In Chapter~\ref{ch:unbounded} we will see how this algorithm may be extended to unbounded synthesis by approximating the winning region during the execution of the algorithm.

\end{itemize}

%%%\section{Case Study: Arbiter}

%%%We consider a simple arbiter system in which the environment makes a request for a number of resources (1 or 2), and the controller may grant access to up to two resources.  The total number of requests grows each round by the number of environment requests and shrinks by the number of resources granted by the controller in the previous round.  The controller must ensure that the number of unhandled requests does not accumulate to more than 2.  Figure~\ref{fig:example} shows the variables (\ref{fig:examplevars}), the initial state of the system (\ref{fig:exampleinit}), and the formulas for computing next-state variable assignments (\ref{fig:exampletrans}) for this example. We use primed identifiers to denote next-state variables and curly braces to define the domain of a variable.

%%%This example is the $n=2$ instance of the more general problem of an arbiter of $n$ resources. For large values of $n$, the set of winning states has no compact representation, which makes the problem hard for BDD solvers. In Section~3 we will outline how the unbounded game can be solved without enumerating all winning states.

%%%\begin{figure}
%%%    \begin{subfigure}[t]{\textwidth}
%%%        \centering
%%%        \begin{tabular}{l | l | l}
%%%            \textbf{Controllable} & \textbf{Uncontrollable} & \textbf{State} \\
%%%            \hline
%%%            \texttt{request : \{1, 2\}} & \texttt{grant0 = \{0, 1\}} & \texttt{resource0 = \{0, 1\}} \\
%%%            & \texttt{grant1 : \{0, 1\}} & \texttt{resource1 = \{0, 1\}} \\
%%%            & & \texttt{nrequests : \{0, 1, 2, 3\}} \\
%%%        \end{tabular}
%%%        \caption{Variables}
%%%        \label{fig:examplevars}
%%%    \end{subfigure}

%%%    \begin{subfigure}[t]{\textwidth}
%%%        \centering
%%%        \texttt{resource0 = 0; resource1 = 0; nrequests = 0;}
%%%        \caption{Initial State}
%%%        \label{fig:exampleinit}
%%%    \end{subfigure}

%%%    \begin{subfigure}[t]{\textwidth}
%%%        \begin {align*}
%%%            \texttt{resource0'} & \texttt{ = grant0;} \\
%%%            \texttt{resource1'} & \texttt{ = grant1;} \\
%%%            \texttt{nrequests'} & \texttt{ = (nrequests + request >= resource0 + resource1)} \\ 
%%%                                & \texttt{ ? (nrequests + request - resource0 - resource1) : 0;}
%%%        \end{align*}
%%%        \caption{Transition Relation}
%%%        \label{fig:exampletrans}
%%%    \end{subfigure}
%%%    \caption{Example}
%%%    \label{fig:example}
%%%\end{figure}


%%%\begin{exmpI}

%%%    %% Make it clear that this is intuition only
%%%    We present the intuition behind our bounded synthesis method by applying
%%%    its \emph{simplified version} to the running example.  We begin by finding
%%%    a trace of length $k$ (here we consider $k=3$) that is winning for the
%%%    controller, i.e., that starts from the initial state and avoids the error
%%%    set for three game rounds (see Figure~\ref{fig:trace}).  We use a SAT
%%%    solver to find such a trace, precisely as one would do in bounded model
%%%    checking.  Given this trace we make an initial conjecture that any trace
%%%    starting with action \texttt{gr0=1 gr1=0} is winning for the controller.
%%%    This conjecture is captured in the abstract game tree shown in
%%%    Figure~\ref{fig:agt}.  We validate this conjecture by searching for a
%%%    counterexample trace that reaches an error state with the first controller
%%%    action fixed to \texttt{gr0=1 gr1=0}.   Such a trace, that refutes the
%%%    conjecture, is shown in Figure~\ref{fig:trace2}.  In this trace, the
%%%    environment wins by playing \texttt{req=2} in the first round.  This move
%%%    represents the environment's partial strategy against the abstract game
%%%    tree in Figure~\ref{fig:agt}.  This partial strategy is shown in
%%%    Figure~\ref{fig:strategy}.
%%%    
%%%    Next we strengthen the abstract game tree taking this partial strategy into account.
%%%    To this end we again use a SAT solver to find a trace where the contoller
%%%    wins while the environment plays according to the partial strategy.  In the
%%%    resulting trace (Figure~\ref{fig:trace3}), the controller plays \texttt{gr0=1 gr1=1} in
%%%    the second round.  We refine the abstract game tree using this move as
%%%    shown in Figure~\ref{fig:refined1}.  The environment's partial strategy was
%%%    to make two requests in the first round, to which the controller responds
%%%    by now granting an additional two resources in the second round.

%%%    When the controller cannot refine the tree by extending existing branches,
%%%    it backtracks and creates new branches. Eventually, we obtain the abstract
%%%    game tree shown in Figure~\ref{fig:refined2} for which there does not exist
%%%    a winning partial strategy on behalf of the environment.  We conclude that
%%%    the bounded game is winning for the controller.

%%%\end{exmpI}

%%%\tikzset{every node/.style={solid}}
%%%\tikzstyle{fixed}=[solid]
%%%\begin{figure}
%%%    \centering
%%%    \captionsetup[subfigure]{width=\textwidth,justification=raggedleft}
%%%    \begin{subfigure}[t]{.4\textwidth}
%%%        \centering
%%%        \begin{minipage}[t][3.8cm][t]{\textwidth}
%%%        \begin{tikzpicture}[level distance = 5mm,baseline]
%%%            \node [circle,draw,inner sep=1pt] (root){}
%%%                child {node [circle,draw,inner sep=1pt] {}
%%%                    child {node [circle,draw,inner sep=1pt] {}
%%%                        child {node [circle,draw,inner sep=1pt] {}
%%%                            child {node [circle,draw,inner sep=1pt] {}
%%%                                child {node [circle,draw,inner sep=1pt] {}
%%%                                    node [left=4pt] {\texttt{gr0=0 gr1=0}}
%%%                                }
%%%                                node [left=4pt] {\texttt{req=1}}
%%%                            }
%%%                            node [left=4pt] {\texttt{gr0=0 gr1=0}}
%%%                        }
%%%                        node [left=4pt] {\texttt{req=1}}
%%%                    }
%%%                node [left=4pt] {\texttt{gr0=1 gr1=0}}
%%%                }
%%%                node [left=4pt] {\texttt{req=1}}
%%%                node [above=4pt] {$\langle s, 3 \rangle$};
%%%        \end{tikzpicture}
%%%    \end{minipage}
%%%        \caption{Controller winning trace}
%%%        \label{fig:trace}
%%%    \end{subfigure}
%%%    \begin{subfigure}[t]{.4\textwidth}
%%%        \centering
%%%        \begin{minipage}[t][3.8cm][t]{\textwidth}
%%%        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm,baseline]
%%%            \node [circle,draw] (root){}
%%%                child {node [circle,draw] {}
%%%                    edge from parent [fixed] node [left] {\texttt{gr0=1 gr1=0}}
%%%                }
%%%                node [left=4pt] {}
%%%                node [above=4pt] {$\langle s, 3 \rangle$};
%%%        \end{tikzpicture}
%%%        \end{minipage}
%%%        \caption{AGT}
%%%        \label{fig:agt}
%%%    \end{subfigure}

%%%    \begin{subfigure}[t]{.4\textwidth}
%%%        \centering
%%%        \begin{minipage}[t][3.8cm][t]{\textwidth}
%%%        \begin{tikzpicture}[level distance = 5mm,baseline]
%%%            \node [circle,draw,inner sep=1pt] (root){}
%%%                child {node [circle,draw,inner sep=1pt] {}
%%%                    child {node [circle,draw,inner sep=1pt] {}
%%%                        child {node [circle,draw,inner sep=1pt] {}
%%%                            child {node [circle,draw,inner sep=1pt] {}
%%%                                child {node [circle,draw,inner sep=1pt] {}
%%%                                    node [left=4pt] {\texttt{gr0=0 gr1=0}}
%%%                                }
%%%                                node [left=4pt] {\texttt{req=1}}
%%%                            }
%%%                            node [left=4pt] {\texttt{gr0=0 gr1=0}}
%%%                        }
%%%                        node [left=4pt] {\texttt{req=1}}
%%%                    }
%%%                node [left=4pt] {\texttt{gr0=1 gr1=0}}
%%%                }
%%%                node [left=4pt] {\texttt{req=2}}
%%%                node [above=4pt] {$\langle s, 3 \rangle$};
%%%        \end{tikzpicture}
%%%        \end{minipage}
%%%        \caption{Environment winning trace}
%%%        \label{fig:trace2}
%%%    \end{subfigure}
%%%    \begin{subfigure}[t]{.4\textwidth}
%%%        \centering
%%%        \begin{minipage}[t][3.8cm][t]{\textwidth}
%%%        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm,baseline]
%%%            \node [circle,draw] (root){}
%%%                child {node [circle,draw] {}
%%%                    edge from parent [fixed] node [left] {\texttt{gr0=1 gr1=0}}
%%%                }
%%%                node [left=4pt] {\texttt{req=2}}
%%%                node [above=4pt] {$\langle s, 3 \rangle$};
%%%        \end{tikzpicture}
%%%        \end{minipage}
%%%        \caption{Partial Strategy}
%%%        \label{fig:strategy}
%%%    \end{subfigure}
%%%    \caption{Abstract game trees.}
%%%    \label{fig:alltrees}
%%%\end{figure}

%%%\begin{figure}
%%%    \centering
%%%    \begin{subfigure}[t]{.2\textwidth}
%%%        \centering
%%%        \begin{minipage}[t][3.8cm][t]{\textwidth}
%%%        \begin{tikzpicture}[level distance = 5mm,baseline]
%%%            \node [circle,draw,inner sep=1pt] (root){}
%%%                child {node [circle,draw,inner sep=1pt] {}
%%%                    child {node [circle,draw,inner sep=1pt] {}
%%%                        child {node [circle,draw,inner sep=1pt] {}
%%%                            child {node [circle,draw,inner sep=1pt] {}
%%%                                child {node [circle,draw,inner sep=1pt] {}
%%%                                    node [left=4pt] {\texttt{gr0=0 gr1=0}}
%%%                                }
%%%                                node [left=4pt] {\texttt{req=1}}
%%%                            }
%%%                            node [left=4pt] {\texttt{gr0=1 gr1=1}}
%%%                        }
%%%                        node [left=4pt] {\texttt{req=1}}
%%%                    }
%%%                node [left=4pt] {\texttt{gr0=1 gr1=0}}
%%%                }
%%%                node [left=4pt] {\texttt{req=2}}
%%%                node [above=4pt] {$\langle s, 3 \rangle$};
%%%        \end{tikzpicture}
%%%        \end{minipage}
%%%        \caption{Controller winning trace}
%%%        \label{fig:trace3}
%%%    \end{subfigure}
%%%    \begin{subfigure}[t]{.3\textwidth}
%%%        \centering
%%%        \begin{minipage}[t][3.8cm][t]{\textwidth}
%%%        \hspace*{0.8cm}
%%%        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm,baseline]
%%%            \node [circle,draw] (root){}
%%%                child {node [circle,draw] {}
%%%                    child {node [circle,draw] {}
%%%                        edge from parent [fixed] node [left] {\texttt{gr0=1 gr1=1}}
%%%                    }
%%%                    edge from parent [fixed] node [left] {\texttt{gr0=1 gr1=0}}
%%%                }
%%%                node [above=4pt] {$\langle s, 3 \rangle$};
%%%        \end{tikzpicture}
%%%        \end{minipage}
%%%        \caption{First refined AGT}
%%%        \label{fig:refined1}
%%%    \end{subfigure}
%%%    \begin{subfigure}[t]{.4\textwidth}
%%%        \centering
%%%        \begin{minipage}[t][3.8cm][t]{\textwidth}
%%%        \begin{tikzpicture}[dash pattern = on 2pt off 2pt, level distance = 10mm,baseline]
%%%            \node [circle,draw] (root){}
%%%                child {node [circle,draw] {}
%%%                    child {node [circle,draw] {}
%%%                        child {node [circle,draw] {}
%%%                            edge from parent [fixed] node [left] {\texttt{gr0=1 gr1=1}}
%%%                        }
%%%                        edge from parent [fixed] node [left] {\texttt{gr0=1 gr1=1}}
%%%                    }
%%%                    edge from parent [fixed] node [left] {\texttt{gr0=1 gr1=0}}
%%%                }
%%%                child {node [circle,draw] {}
%%%                    child {node [circle,draw] {}
%%%                        child {node [circle,draw] {}
%%%                            edge from parent [fixed] node [right] {\texttt{gr0=1 gr1=1}}
%%%                        }
%%%                        edge from parent [fixed] node [right] {\texttt{gr0=1 gr1=1}}
%%%                    }
%%%                    edge from parent [fixed] node [right] {\texttt{gr0=1 gr1=1}}
%%%                }
%%%                node [above=4pt] {$\langle s, 3 \rangle$};
%%%        \end{tikzpicture}
%%%        \end{minipage}
%%%        \caption{Final Refined AGT}
%%%        \label{fig:refined2}
%%%    \end{subfigure}
%%%    \caption{Refined abstract game trees.}
%%%    \label{fig:refinedtrees}
%%%\end{figure}
