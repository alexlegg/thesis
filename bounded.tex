\chapter{Bounded Realisability}
\label{ch:bounded}

\newtheorem*{exmp}{Example}
\newtheorem*{exmpI}{Example: Intuition behind the algorithm}

In this chapter I will describe my work on bounded realisability of reactive systems with safety properties. As introduced in Chapter~\ref{ch:background} reactive realisability is the problem of determining the existence of a program, which we call a \emph{controller}, that continuously interacts with its environment in adherence with a specification. A safety property is a simple condition that defines a set of \emph{error states} that the controller must avoid in order to be correct.

Realisability is the first step on the path to synthesis. In the subsequent chapter I will describe an algorithm that extracts the actions of the controller necessary for realisation. This strategy may be used for synthesis: automatic construction of the controller program. Reactive synthesis for controllers with safety properties has many practical uses in areas such as circuit design, device drivers, or industrial automation.

The algorithm described in this chapter solves bounded safety games. Recall that Chapter~\ref{ch:background} introduced games as a formalism for synthesis by stating the problem in terms of a game between a controller and its environment. In this chapter we are concerned with \emph{bounded} games that restrict all runs in the game to certain length. This concept is borrowed from model checking where it is used to verify that a program emits no erroneous traces of a certain length. This verification is actualised by the construction of a propositional formula that is satisfiable when a trace that visits an error state exists in the model. A SAT solver can be used to efficiently search for a satisfying assignment to this formula, which represents a counterexample to the correctness property of the specification. 

In the case of realisability, the existence of a trace that reaches an error state does not guarantee that the specification is unrealisable.  Instead we are interested in player \emph{strategies}. A controller strategy must avoid the error states for all possible environment actions. Likewise, an environment strategy must take into account all controller actions. We cannot use a SAT solver to search for a strategy directly since we require both existential and universal quantifiers. We can, however, check if a strategy allows a counterexample trace without quantification. This sets us up for a counterexample guided methodology in which we construct candidate strategies and check them for correctness. If we discover a counterexample we use it to guide a refinement step in which we improve the candidate strategy.

Similar to bounded model checking, bounded realisability does not guarantee unbounded realisability.  If we decide that the controller can avoid error states for a game bounded to $k$ rounds there is no guarantee that the environment can not force an error in a game with a bound higher than $k$. In Chapter~\ref{ch:unbounded} I present an extension to the algorithm that is complete for unbounded games.

Restricting ourselves to solving a bounded safety game enables us to turn the focus of the algorithm from states to traces. The traditional approach of constructing a binary decision diagram to symbolically represent the winning region has the potential to consume exponential space. The advantage of concentrating on runs of the game is that we do not rely on computing the winning states and therefore do not suffer from the related state explosion. The factors affecting the upper limit on scalability for the bounded synthesis algorithm are different to those of the BDD based approach. The most efficient algorithm for a particular realisability problem depends on the properties of that problem instance.

\section{Algorithm}

This work draws inspiration from a QBF solving algorithm that treats the QBF problem as a game~\cite{Janota12}. In that algorithm one player assumes the role of the universal quantifiers and the opponent takes on the existential quantifiers. In the game, the players take turns to chooses values for their variables from the outermost quantifier block in. Quantifiers may be removed from a formula by iteratively constructing and merging copies of the formula for each quantified variable. The copies of the formula represent the two possible values, true or false, of the quantified boolean variable. Universal quantification can then be reduced to the conjunction of these copies, and existential quantifications corresponds to a disjunction. In practice, these expanded formulas are far too large to be solved so the authors introduce abstractions, or partially expanded formulas, to avoid expanding on variables unnecessarily. The abstractions are refined through a CEGAR process of searching for candidate solutions and analysing counterexamples. The full algorithm is described in detail in Chapter~\ref{ch:relatedwork}. 

We define a safety game by the tuple $G = (\cS, \cU, \cC, \delta, s_0, E)$. $S$, $\cU$, and $\cC$ are sets of boolean variables representing game states, environment actions, and controller actions respectively. The transition relation, $\delta$, is a boolean formula $\delta : 2^{S} \times 2^\cU \times 2^\cC \to 2^\cS$ that maps current states and actions to successor states. The game begins in the initial state $s_0$ and the $E$ is the set of error states that the controller must avoid.  A controller strategy is a function $\pi^c : 2^\cS \times 2^\cU \to 2^\cC$, i.e. a mapping from states and environment actions to controller actions. We say that a strategy is winning if all runs in which the controller chooses actions according to $\pi^c$ avoid error states. If $\pi^c$ is a partial function that defines a mapping for a subset of $2^\cS \times 2^\cU$ to $2^\cC$ then we call $\pi^c$ a partial strategy.

Realisability is the problem to determining the existence of a winning controller strategy.  The following quantified formula may be used to solve realisability of a safety game bounded to $k$ game rounds: 
\begin{equation*}
\begin{multlined}
    \forall \cU_k \exists \cC_k \forall \cU_{k-1} \exists \cC_{k-1} \ldots \forall \cU_0 \exists \cC_0 \big( s_0(\cS_k) \land \\
    \lnot E(\cS_k) \land \delta(\cS_k, \cU_k, \cC_k, \cS_{k-1}) \land \ldots \land \lnot E(\cS_1) \land \delta(\cS_1, \cU_1, \cC_1, \cS_0) \land \lnot E(\cS_0) \big).
\end{multlined}
\end{equation*}

The formula is constructed by unrolling the transition relationship for every game round until the bound is reached. A quantifier alternation is introduced to the formula for the variables corresponding to the actions of each player. Universal quantifiers are used for the environment variables and existential for the controller. The formula is constrained so that if the state at any game round is an error state the formula evaluates to false. Hence it is satisfiable if and only if a strategy exists for the controller that avoids the error states.

It is possible to solve the QBF na\"ively but we can do better by taking into account structural information in the realisability problem that is lost in the translation to prenex normal form. In the remainder of this chapter I describe an adaptation of the counterexample guided QBF algorithm that takes advantage of this information.

%%%In particular, awareness that the formula is constructed of a repeated transition relation enables more effective learning. A na\"ive solver may learn sets of actions in order to prune the search tree. Instead we learn sets of states and, where possible, we transmit learned information between game rounds. For example, if we discover that the environment can force a visit to an error state from some set of states $s$ in $n$ rounds we also know that the environment can win from $s$ in $n+m$ rounds for all $m > 0$. Knowledge of how to construct the formula also enables the solver to efficiently produce a smaller expanded formula when necessary by simply copying only the parts of the formula corresponding to game rounds after the action of the expanded quantifier.

\subsection{Example}
\label{sec:boundedexample}

We introduce an example to assist an intuitive explanation of the algorithm. Consider a model of a simple storage device driver. The operating system makes requests of the driver to write or read data to or from the device. The structure of the model is shown in Figure~\ref{fig:exampleStructural}. It is the role of the driver to grant these requests while ensuring that a \texttt{read} never occurs when a \texttt{write} was requests and vice versa.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \node [minimum width=3.5cm,minimum height=1cm,draw] (osRoot){Operating System};
        \node [minimum width=3.5cm,minimum height=1cm,draw,below =of osRoot] (drRoot){Driver};
        \node [minimum width=3.5cm,minimum height=1cm,draw,below=of drRoot] (devRoot){Device};

        \draw [->] (osRoot) -- node [right] {\texttt{os\_request}} (drRoot);
        \draw [->] (drRoot) -- node [right] {\texttt{dev\_cmd}} (devRoot);

    \end{tikzpicture}
    \caption{Structure of device driver example}
    \label{fig:exampleStructural}
\end{figure}

As detailed in Chapter~\ref{ch:background}, we formalise realisability by a game structure $G = (\cS, \cU, \cC, \delta, s_0)$. The structure for our example is:

\begin{itemize}
\item $\cS = \{ \texttt{request}, \texttt{error} \} $. The game consists of two boolean variables to denote the current request from the OS to the driver, and whether an error has occurred. We use $\texttt{request} = 0$ to represent a \texttt{read} and $\texttt{request} = 1$ for \texttt{write}.
    \item $\cU = \{ \texttt{os\_request} \} $. The uncontrollable actions consist of a single boolean variable to describe a \texttt{read} or a \texttt{write}. We use the same values as before, 0 for \texttt{read} and 1 for \texttt{write}.
    \item $\cC = \{ \texttt{dev\_cmd} \}$. The controllable actions similarly consists of a single boolean variable to denote the command given to the device: a \texttt{read} ($\texttt{dev\_cmd} = 0$) or a \texttt{write} ($\texttt{dev\_cmd} = 1$).
    \item The transition relation $\delta$ is defined by the following formulas:

        $$ \texttt{request'} \gets \texttt{os\_request} $$

        $$ \texttt{err'} \gets \texttt{request} \neq \texttt{dev\_cmd} $$

        Primed variables are used here to indicate how the value is assigned in the next game round.

\item $s_0 = (\texttt{request} = 0 \land \texttt{err} = 0)$. To simplify the example there is no idle state so the model is initialised with a pending \texttt{read} request.

\end{itemize}

\tikzset{
    pil/.style={
        -{Latex[length=3mm]},
        thick
    },
    snode/.style={
        align=center,
        circle,
        draw,
        thick
    }
}

\begin{figure}[b]
    \centering
    \begin{tikzpicture}[level distance = 5mm,baseline]
            \node [snode] (n00){(0, 0)};
            \node [snode,right=2cm of n00] (n10){(1, 0)};
            \node [snode,accepting,below=of n00] (n01){(0, 1)};
            \node [snode,accepting,below=of n10] (n11){(1, 1)};
            \node [above=of n00] (init){};

            \draw [pil] (n00) edge [bend left] node [above] {(1, 0)} (n10);
            \draw [pil] (n10) edge [bend left] node [below] {(0, 1)} (n00);

            \draw [pil] (n00) edge [loop left] node [left] {(0, 0)} (n00);
            \draw [pil] (n10) edge [loop right] node [right] {(1, 1)} (n10);

            \draw [pil] (n00) edge node [left] {(*, 1)} (n01);
            \draw [pil] (n10) edge node [right] {(*, 0)} (n11);

            \draw [pil] (init) edge (n00);
    \end{tikzpicture}
    \caption[State automata representation of example $\delta$.]{State automata representation of $\delta$ in Example 1. Nodes are labelled by the tuple (\texttt{request}, \texttt{error}). Edges are labelled with uncontrollable and controllable actions: (\texttt{os\_request}, \texttt{dev\_cmd}). A star indicates that the transition occurs on both a 0 and a 1. Transitions from error states are elided for simplicity.}
    \label{fig:example1}
\end{figure}

The bounded synthesis algorithm is set within a counterexample guided abstraction refinement framework. An abstraction serves a dual purpose in this approach as both a representation of a player strategy and as a way to reduce the search space of the game. This is achieved by employing one player's candidate strategy as its opponent's game abstraction. The effect is that the search for a player's strategy is directed by its opponent's current best effort strategy. Intuitively both players escalate their strategies until one of them converges on a winning strategy.

The abstractions of the game that we construct during the CEGAR search restrict actions available to one of the players.  Specifically, we consider abstractions represented as trees of actions, referred to as \emph{abstract game trees} (AGTs).  Figure~\ref{fig:example1b} shows an example abstract game tree restricting the environment (abstract game trees restricting the controller are similar).  The root of the tree is labelled with an initial state and a game length.  In the abstract game, the controller can freely choose actions whilst the environment is required to pick actions from the tree.  After reaching a leaf, the environment continues playing unrestricted.  The tree in Figure~\ref{fig:example1b} restricts the first environment action to $\texttt{os\_request} = 1$. At the leaf of the tree the game continues unrestricted.

\captionsetup[subfigure]{singlelinecheck=off,justification=centering}
\tikzset{every node/.style={solid}}
\tikzset{align at top/.style={baseline=(current bounding box.north)}}
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw,inner sep=1pt] (root){}
                child {node [circle,draw,inner sep=1pt] {}
                    child {node [circle,draw,inner sep=1pt] {}
                        child {node [circle,draw,inner sep=1pt] {}
                            node [left=4pt] {$\texttt{cmd} = 0$}
                        }
                        node [left=4pt] {$\texttt{req} = 1$}
                    }
                node [left=4pt] {$\texttt{cmd} = 1$}
                }
                node [left=4pt] {$\texttt{req} = 1$}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Trace to error}
        \label{fig:example1a}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    edge from parent node [left] {$\texttt{req} = 1$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Candidate environment strategy}
        \label{fig:example1b}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw,inner sep=1pt] (root){}
                child {node [circle,draw,inner sep=1pt] {}
                    child {node [circle,draw,inner sep=1pt] {}
                        child {node [circle,draw,inner sep=1pt] {}
                            node [left=4pt] {$\texttt{cmd} = 1$}
                        }
                        node [left=4pt] {$\texttt{req} = 1$}
                    }
                node [left=4pt] {$\texttt{cmd} = 0$}
                }
                node [left=4pt] {$\texttt{req} = 1$}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Counterexample trace}
        \label{fig:example1c}
    \end{subfigure}
    \captionsetup{justification=centering}
    \caption[Execution of bounded realisability on the example.]{Execution of bounded realisability on the example.\\Here we use abbreviations \texttt{req} for \texttt{os\_request} and \texttt{cmd} for \texttt{dev\_cmd}.\\Root nodes are labelled with initial state $(\texttt{os\_request}, \texttt{dev\_cmd})$ and a game length.}
    \label{fig:example1exe}
\end{figure}

We will now step through an execution of the algorithm using the example just introduced. The first step involves a search of the empty game abstraction for an initial candidate strategy for the environment player. In the empty abstraction we have not yet restricted the game in any way so all runs through the game are enabled. We search for a candidate strategy by finding a run that reaches an error state. Here we are only searching for the existence of a run and so we do not require quantifier alternations and a SAT solver can be used to efficiently perform the search. Intuitively, an existential search is equivalent to the two players of the game cooperating.  Effectively, \textsc{findCandidate} employs cooperation as a heuristic for optimistically discovering candidate strategies or alternatively quickly discovering useful counterexamples that speed up the refinement loop. The heuristic is based on the observation that most real world systems to which synthesis may be applied are designed to allow the implementation of an efficient controller.  Figure~\ref{fig:example1a} shows a trace through the example game that reaches an error state.

The trace informs us that by playing the actions contained in the trace it is possible for the environment to reach an error state. From this we conjecture that the first action in the trace is a reasonable choice for the first move in the environment's winning strategy. So we construct a candidate strategy in which the environment plays $\texttt{os\_request} =1$ in the first game round. The next step is to validate our conjecture by searching for counterexamples. We do this by constructing a new abstraction of the game in which the environment is restricted to playing actions from its candidate strategy (Figure~\ref{fig:example1b}). Then we play this abstract game on the behalf of the controller. Once again we search for a trace through the game but this time the SAT solver is searching for a trace that avoids error states for the duration of the bounded game. Any traces found in this way indicate the possibility of a spoiling strategy for the controller that defeats the candidate strategy of the environment. Figure~\ref{fig:example1c} shows a trace in which the controller counters the environment by playing $\texttt{dev\_cmd} = 0$ in the first round to match the initial state of $\texttt{request} = 0$.

We can define a partial strategy for an abstract game by labelling the nodes in the tree to define the actions a player should choose against the opponent's actions in the edges of the tree. The trace contains controller actions that can be used to form a counterexample partial strategy (Figure~\ref{fig:example1d}).  Our goal now is to refine the candidate strategy for the environment so that it wins against the controller's partial strategy. In the candidate strategy we have not yet selected an environment action for the second game round, so we may refine the strategy by doing this now. Since the game is deterministic there is a unique state $(\texttt{request} = 1 \land \texttt{err} = 0)$ reachable by playing the actions in the combination of AGT and counterexample strategy.  We now solve an abstract game with a bound of 1 from this state in order to determine which action the environment should select for the second game round. 

This new game is solved via a recursive call to the algorithm. First a trace to an error state is found (Figure~\ref{fig:example1e}) and an environment candidate strategy is constructed (Figure~\ref{fig:example1j}) from the first environment action in the trace. Then a counterexample trace to the environment's strategy is found in which the controller chooses correctly to play $\texttt{dev\_cmd} = 1$.  At this point it is impossible to refine the environment candidate strategy by appending additional actions because the bound on game length has been reached. Instead an action from the candidate is backtracked and the search continues on a refined AGT that now includes the counterexample (Figure~\ref{fig:example1f}).


\begin{figure}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][2.2cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    node [left=4pt] {$\texttt{cmd} = 0$}
                    edge from parent node [left] {$\texttt{req} = 1$}
                }
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Partial strategy}
        \label{fig:example1d}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][2.2cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw,inner sep=1pt] (root){}
                child {node [circle,draw,inner sep=1pt] {}
                    node [left=4pt] {$\texttt{cmd} = 0$}
                }
                node [left=4pt] {$\texttt{req} = 1$}
                node [above=4pt] {$\langle (1, 0), 1 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Trace to error}
        \label{fig:example1e}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][2.2cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    edge from parent node [left] {$\texttt{req} = 1$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (1, 0), 1 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{AGT with controller actions}
        \label{fig:example1j}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][2.2cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    edge from parent node [left] {$\texttt{cmd} = 1$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (1, 0), 1 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Refined AGT}
        \label{fig:example1f}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][2.2cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    edge from parent node [left] {$\texttt{cmd} = 0$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Refined AGT}
        \label{fig:example1g}
    \end{subfigure}%
    \caption{Continued example algorithm execution}
    \label{fig:example1execont}
\end{figure}

The environment is now playing against a restricted opponent. In this case there is no possible action that the environment can take to reach an error state. This can be seen in the state machine for the game in Figure~\ref{fig:example1}. 
None of the outgoing transitions from $(1, 0)$ with the controller choosing to play $\texttt{dev\_cmd} = 1$ lead to an error state in one step.  If the environment cannot win from this state against a restricted controller then clearly it cannot win against an unrestricted controller in the game bounded to one round. We can therefore conclude that the candidate strategy that led to this state was not a winning strategy for its abstract game.

Actually we may conclude a stronger assertion that any strategy that results in this state with one game round remaining is a bad strategy. It is possible to exclude these strategies from future searches in an optimisation described in Section~\ref{sec:boundedLearning}.

The algorithm now backtracks to the very beginning. We have determined that the candidate environment strategy in Figure~\ref{fig:example1b} can be defeated by the partial controller strategy in Figure~\ref{fig:example1d}. Now the abstraction of the game is refined to include the counterexample. The original empty abstraction refined with a single counterexample action is shown in Figure~\ref{fig:example1g}. Note that a trace reaching an error in which the environment plays $\texttt{os\_request} = 1$ is still possible in this abstract game (Figure~\ref{fig:example1k}). The algorithm without optimisation may consider this candidate again for the new abstract game. The result will be the same and refinement occurs again, except now the game is refined to include a counterexample action from the second round (Figure~\ref{fig:example1h}). On this game abstraction the candidate is blocked because the trace must include $\texttt{dev\_cmd} = 1$ in the second round.

As a result the environment must choose a different action for the first round of the game. A SAT query will reveal that $\texttt{os\_request} = 0$ can lead to an error when the controller plays $\texttt{dev\_cmd} = 1$ in the second round. However, this candidate can be defeated by the controller choosing $\texttt{dev\_cmd} = 0$ instead. The algorithm will discover this and refine the abstraction again to include the counterexample. In refined abstract game (Figure~\ref{fig:example1i}) the environment has no winning trace and therefore the algorithm terminates and returns realisable for the controller. The final game tree serves as a \emph{certificate tree} that proves the nonexistence of an environment strategy. In the next chapter I show how a controller strategy can be extracted from a certificate tree.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw,inner sep=1pt] (root){}
                child {node [circle,draw,inner sep=1pt] {}
                    child {node [circle,draw,inner sep=1pt] {}
                        child {node [circle,draw,inner sep=1pt] {}
                            node [left=4pt] {$\texttt{cmd} = 0$}
                        }
                        node [left=4pt] {$\texttt{req} = 1$}
                    }
                node [left=4pt] {$\texttt{cmd} = 0$}
                }
                node [left=4pt] {$\texttt{req} = 1$}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Trace to error}
        \label{fig:example1k}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {$\texttt{cmd} = 1$}
                    }
                    edge from parent node [left] {$\texttt{cmd} = 0$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Refined AGT}
        \label{fig:example1h}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {$\texttt{cmd} = 1$}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {$\texttt{cmd} = 0$}
                    }
                    edge from parent node [left] {$\texttt{cmd} = 0$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle (0, 0), 2 \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Refined AGT}
        \label{fig:example1i}
    \end{subfigure}%
    \caption{Continued example algorithm execution (2)}
    \label{fig:example1execont2}
\end{figure}


\subsection{Abstract game trees}

An abstract game tree (AGT) is a restricted version of the concrete game in which fewer actions are available to one of the players. For example, in Figure~\ref{fig:example1i} the controller is restricted to playing $\texttt{dev\_cmd} = 0$ in the first round and either $\texttt{dev\_cmd} = 0$ or $\texttt{dev\_cmd} = 1$ in the second.  The root of the tree is annotated by the initial state $s$ of the abstract game and the bound $k$ on the number of rounds.  We denote $\textsc{nodes}(T)$ the set of all nodes of a tree $T$, $\textsc{leaves}(T)$ the subset of leaf nodes.  For edge $e$, $\textsc{action}(e)$ is the action that labels the edge, and for node $n$, $\textsc{height}(k, n)$ is the distance from $n$ to the last round of a game bounded to $k$ rounds. Figure~\ref{fig:agtHeight} shows how \textsc{height} is calculated, $n_1$ has a height of two because the three round game continues for one additional round after the leaf nodes.  $\textsc{height}(k, T)$ is the height of the root node of the tree.  For node $n$ of the tree, $\textsc{succ}(n)$ is the set of pairs $\langle e, n' \rangle$ where $n'$ is a child node of $n$ and $e$ is the edge connecting $n$ and $n'$.

\begin{figure}
    \centering
    \begin{tikzpicture}[level distance = 10mm]
        \node [circle,draw] (root){}
            child {node [circle,draw] (n1) {}
                child {node [circle,draw] {}
                    node [left=4pt] {$n_2$}
                    edge from parent node [left] {$\texttt{cmd} = 1$}
                }
                child {node [circle,draw] (n3) {}
                    child {node [] (n4) {}
                        node [left=4pt] {}
                        node [right=1.75cm] (n4line) {$0$}
                        edge from parent [draw=none]
                    }
                    node [left=4pt] {$n_3$}
                    node [right=1.75cm] (n3line) {$1$}
                    edge from parent node [right] {$\texttt{cmd} = 0$}
                }
                node [left=4pt] {$n_1$}
                node [right=2.5cm] (n1line) {$2$}
                edge from parent node [left] {$\texttt{cmd} = 0$}
            }
            node [left=4pt] {$T$}
            node [right=2.5cm] (n0line) {$3$}
            node [above=4pt] {$\langle (0, 0), 3 \rangle$};


        \draw (root) to (n0line);
        \draw (n1) to (n1line);
        \draw (n3) to (n3line);
        \draw (n4) to (n4line);
    \end{tikzpicture}
    \caption{Height of an AGT node}
    \label{fig:agtHeight}
\end{figure}

Given an environment (controller) abstract game tree $T$ a \emph{partial strategy} $Strat: \textsc{nodes}(T) \rightarrow 2^\cC$ ($Strat: \textsc{nodes}(T) \rightarrow 2^\cU$) labels each node $n$ of the tree with the controller's (environment's) action to be played in the game round corresponding to $\textsc{height}(k, n)$. Figure~\ref{fig:example1d} shows a partial strategy constructed during the example execution. In the example an action for the controller ($\texttt{dev\_cmd} = 0$) is defined as the action to play in the first round of the game against the environment's first action ($\texttt{os\_request} = 1$).  

Given a partial strategy $Strat$, we can map each leaf $l$ of the abstract game tree to $\langle s',i'\rangle=\textsc{outcome}(\langle s, i\rangle, Strat, l)$ obtained by playing all controllable and uncontrollable actions on the path from the root to the leaf.  An environment (controller) partial strategy is \emph{winning against $T$} if all its outcomes are states that are winning for the environment (controller) in the concrete game.

\subsection{Counterexample guided realisability}

The bounded realisability algorithm constructs candidate strategies for one player that serve the dual purpose of game abstraction for its opponent. The algorithm begins by discovering a candidate for the environment. Next we must determine if there are counterexamples to the candidate. This step is executed by constructing an abstract game tree from the environment's candidate strategy and recursively invoking the algorithm on this new abstraction. The recursive call plays against the environment's strategy on behalf of the controller. Thus the algorithm can be seen as running two competing solvers, for the controller and for the environment. By symmetrically playing for both players we achieve the goal of directing the search towards strong strategies and counterexamples.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{solveAbstract}{$p, s, k, T$}
        \State $cand \gets $ \Call{findCandidate}{$p, s, k, T$} \Comment{Look for a candidate}\label{line:findcandidate}
        \IIf{$k = 1$} \Return $cand$ \EndIIf \Comment{Reached the bound}
        \State $T' \gets T$
        \Loop
            \If{$cand = \texttt{NULL}$} \Comment{No candidate: return with no solution}
                \State \Return $\texttt{NULL}$
            \EndIf 
            \State $\langle cex, l, u \rangle \gets $ \Call{verify}{$p, s, k, T, cand$} \label{line:callVerify} \Comment{Verify candidate}
            \If{$cex = \False$} \Comment{No counterexample: return candidate}
                \State \Return $cand$ 
            \EndIf 
            \State $T' \gets $ \Call{append}{$T', l, u$} \Comment{Refine $T'$ with counterexample}
            \State $cand \gets $ \Call{solveAbstract}{$p, s, k, T'$} \Comment{Solve refined game tree}
        \EndLoop
        \EndFunction
        \algstore{b1}
    \end{algorithmic}
    \caption{Solve an abstract bounded game}
    \label{alg:solveabstract}
\end{algorithm}


The full procedure is illustrated in \Cref{alg:solveabstract,alg:findcandidate,alg:verify}. \textsc{solveAbstract} takes a concrete game $G$ with maximum bound $\kappa$ as an implicit argument.  In addition, it takes a player $p$ (controller or environment), state $s$, bound $k$ and an abstract game tree $T$ and returns a winning partial strategy for $p$, if one exists.  The initial invocation of the algorithm takes the initial state $I$, bound $\kappa$ and an empty abstract game tree $\emptyset$.  Initially the solver is playing on behalf of the environment since that player takes the first move in every game round.  The empty game tree does not constrain opponent moves, hence solving such an abstraction is equivalent to solving the original concrete game.  

The algorithm is organised as a counterexample-guided abstraction refinement (CEGAR) loop.  The first step of the algorithm uses the \textsc{findCandidate} function, described below, to come up with a candidate partial strategy that is winning when the opponent is restricted to $T$.  If it fails to find a strategy, this means that no winning partial strategy exists against the opponent playing according to $T$.  If, on the other hand, a candidate partial strategy is found, we need to verify if it is indeed winning for the abstract game $T$.

The \textsc{verify} procedure searches for a \emph{spoiling} counterexample strategy by solving new games beginning at the outcome in each leaf of the AGT after applying the candidate strategy. The new games are solved by a recursive call to \textsc{solveAbstract}, which now plays on behalf of the opponent. The dual solver is searching for a continuation of the current game that ensures that the opponent always wins.

\begin{algorithm}
    \begin{algorithmic}
        \algrestore{b1}
        \Function{findCandidate}{$p, s, k, T$}
        \State $\hat{T} \gets $ \Call{extend}{$T$} \Comment{Extend the tree with arbitrary actions}\label{line:extend}
            \If{$p = \texttt{cont}$}
                \State $f \gets $ \Call{treeFormula}{$k, \hat{T}$}
            \Else
                \State $f \gets $ \Call{\textoverline{treeFormula}}{$k, \hat{T}$}
            \EndIf
            \State $sol \gets $ \Call{SAT}{$s(\cS_{\hat{T}}) \land f$}
            \If{$sol = \texttt{unsat}$} 
                \State $\Call{learn}{p, s, k, T, f}$\label{line:boundedLearning}
                \Comment{This line is enabled in an optimisation}
                \State \Return $\texttt{NULL}$ \Comment{No candidate exists}
            \Else
                \LineComment{Return partial strategy for $T$}
                \State \Return $\{ \langle n, c \rangle\ |\ n \in $ \Call{nodes}{$T$}$, c = sol(n) \}$
            \EndIf
        \EndFunction
        \algstore{b2}
    \end{algorithmic}
    \caption{Find a candidate strategy}
    \label{alg:findcandidate}
\end{algorithm}


\begin{algorithm}
    \begin{algorithmic}
        \algrestore{b2}
        \Function{verify}{$p, s, k, T, cand$}
            \For{$l \in \Call{leaves}{T}$}
            \State $\langle k', s'\rangle \gets $ \Call{outcome}{$s, k, cand, l$} \Comment{Get bound and state at leaf}
            \IIf{$k' = 0$} \Continue \EndIIf
            \If{$p = \textsc{cont}$}
                \State $T' \gets \emptyset$
            \Else
                \State $T' \gets \{ cand(l) \}$
            \EndIf
                \LineComment{Solve for the opponent}
                \State $a \gets $ \Call{solveAbstract}{\Call{opponent}{$p$}, $s'$, $k'$, $T'$} 
                \IIf{$a \neq \texttt{NULL}$} \Return $\langle \True, l, a \rangle$ \EndIIf \Comment{Return counterexample}
            \EndFor
            \State \Return $\langle \False, \emptyset, \emptyset \rangle$ \Comment{There was no counterexample}
        \EndFunction
    \end{algorithmic}

    \caption{Verify a candidate strategy}
    \label{alg:verify}
\end{algorithm}

If the dual solver can find no spoiling strategy at any of the leaves then the candidate contains the prefix of a winning strategy for the abstract game.  Otherwise, \textsc{verify} returns the action used by the opponent after the leaf of the AGT in a spoiling strategy. The abstract game is refined by appending this action to the corresponding leaf in $T$ in line~\ref{line:callVerify}.

We solve the refined game by recursively invoking \textsc{solveAbstract} on it.  If no partial winning strategy is found for the refined game then there is also no partial winning strategy for the original abstract game, and the algorithm returns a failure.  Otherwise, the partial strategy for the refined game is \emph{projected} on the original abstract game by removing the leaves introduced by refinements (see Figure~\ref{fig:projection}). The resulting partial strategy becomes a candidate strategy to be verified at the next iteration of the loop. In the worst case the loop terminates after all actions in the game are refined into the abstract game.  

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \centering
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {$u_1$}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {$u_2$}
                    }
                    edge from parent node [left] {$u_0$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle s, k \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Original AGT}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \centering
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        child {node [circle,draw] {}
                            node [left=4pt] {$c_3$}
                            edge from parent node [right] {$u_3$}
                        }
                        node [left=4pt] {$c_1$}
                        edge from parent node [left] {$u_1$}
                    }
                    child {node [circle,draw] {}
                        child {node [circle,draw] {}
                            node [right=4pt] {$c_4$}
                            edge from parent node [right] {$u_4$}
                        }
                        node [right=4pt] {$c_2$}
                        edge from parent node [right] {$u_2$}
                    }
                    node [left=4pt] {$c_0$}
                    edge from parent node [left] {$u_0$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle s, k \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \captionsetup{justification=centering}
        \caption{Refined AGT with candidate strategy}
    \end{subfigure}%
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \begin{minipage}[t][3.9cm][t]{\textwidth}
        \centering
        \begin{tikzpicture}[align at top, level distance = 10mm]
            \node [circle,draw] (root){}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        node [left=4pt] {$c_1$}
                        edge from parent node [left] {$u_1$}
                    }
                    child {node [circle,draw] {}
                        node [right=4pt] {$c_2$}
                        edge from parent node [right] {$u_2$}
                    }
                    node [left=4pt] {$c_0$}
                    edge from parent node [left] {$u_0$}
                }
                node [left=4pt] {}
                node [above=4pt] {$\langle s, k \rangle$};
        \end{tikzpicture}
        \end{minipage}
        \caption{Projected strategy}
    \end{subfigure}%
    \caption{Projection of a candidate strategy}
    \label{fig:projection}
\end{figure}

The CEGAR loop depends on the ability to guess candidate partial strategies in \textsc{findCandidate}. For this purpose we use the heuristic that a partial strategy may be winning if each \textsc{outcome} of the strategy can be extended to a run of the game that is winning for the current player.  Clearly, if such a partial strategy does not exist then no winning partial strategy can exist for the abstract game tree. We formulate this heuristic as a SAT query such that any satisfying assignment encodes such a strategy. The query is constructed recursively by $\textsc{treeFormula}$ (for the controller) or $\textsc{\textoverline{treeFormula}}$ (for the environment) in Algorithm~\ref{alg:treeFormula}.  

The tree is first extended to the full height of the game with edges that are labeled with arbitrary opponent actions (Algorithm~\ref{alg:findcandidate}, line~\ref{line:extend}).  For each node in the tree, new SAT variables are introduced corresponding to the state ($\cS_T$) and action ($\cU_T$ or $\cC_T$) variables of that node. Additional variables for the opponent actions in the edges of $T$ are introduced ($U_e$ or $C_e$) and set to $\textsc{action}(e)$.  The state and action variables of node $n$ are connected to successor nodes $\textsc{succ}(n)$ by an encoding of the transition relation and constrained to the winning condition of the player.  

\begin{algorithm}
    \caption{Tree formulas for Controller and Environment}
    \label{alg:treeFormula}
    \begin{algorithmic}[1]
        \Function{treeFormula}{$k, T$}
        \If{$\Call{height}{k, T} = 0$}
        \State \Return{ $\lnot \Call{E}{\cS_{T}}$ }
        \Else
        \State \Return{$\lnot \Call{E}{\cS_{T}} \land$ \\
            $$\bigwedge_{\langle e, n \rangle \in \Call{succ}{T}}(\Call{$\delta$}{\cS_T, \cU_e, \cC_T, \cS_n} \land U_e = \Call{action}{e} \land \Call{treeFormula}{k, n})$$
        }
        \EndIf
        \EndFunction
        \algstore{tf1}
    \end{algorithmic}

    \begin{algorithmic}[1]
        \algrestore{tf1}
        \Function{\textoverline{treeFormula}}{$k, T$}
        \If{$\Call{height}{k, T} = 0$}
        \State \Return{\Call{E}{$\cS_{T}$}}
        \Else
        \State \Return{ $\Call{E}{\cS_{T}} \lor$ \\
        $$\bigvee_{\langle e, n \rangle \in \Call{succ}{T}}(\Call{$\delta$}{\cS_T, \cU_T, \cC_e, \cS_n} \land C_e = \Call{action}{e} \land \Call{\textoverline{treeFormula}}{k, n})$$ }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Correctness}

Completeness of the algorithm follows from the completeness of the backtracking search. In the worst case the algorithm will construct the entire concrete game tree and effectively expand all quantifiers. Soundness follows from the existential search of the SAT solver in \textsc{findCandidate}. The algorithm terminates after searching for a candidate strategy on an abstract game tree with actions fixed only for the opponent. If no candidate can be found with the opponent restricted in this way then no strategy exists for the player.

\begin{proposition}\label{prop:findcandidate}
    Let $T$ be an abstract game tree with edges labelled by opponent actions. If $\textsc{findCandidate}(p, s, k, T) = \texttt{NULL}$ then there is no strategy for $p$ to win a game bounded to $k$ rounds from $s$.
\end{proposition}
\begin{proof}
    We assume that $p$ is the controller, the proof for the environment is similar. The first line of $\textsc{findCandidate}$ constructs $\hat{T}$ by extending $T$ to the full length of the game with arbitrary environment actions. The algorithm then proceeds to construct $f = \textsc{treeFormula}(k, \hat{T})$ and query a SAT solver for a satisfying assignment to $s(\cS_k) \land f$. The algorithm returns \texttt{NULL} when there is no satisfying assignment.


    At every iteration of $\textsc{treeFormula}$, a conjunction expands a universal quantifier into a subset of its possible values: the actions in the tree. Therefore, $s(\cS_k) \land f$ is a partial expansion of the formula representing the existence of a controller strategy:
    \begin{equation*}
    \begin{multlined}
    \forall \cU_k \exists \cC_k \forall \cU_{k-1} \exists \cC_{k-1} \ldots \forall \cU_0 \exists \cC_0 \big( s_0(\cS_k) \land \\
    \lnot E(\cS_k) \land \delta(\cS_k, \cU_k, \cC_k, \cS_{k-1}) \land \ldots \land \lnot E(\cS_1) \land \delta(\cS_1, \cU_1, \cC_1, \cS_0) \land \lnot E(\cS_0) \big).
    \end{multlined}
    \end{equation*}
    The partial expansion is satisfiable when there is a corresponding assignment to $\cC_k \dots \cC_1$ for every value in the subset of expanded environment variables.  If there is not an assignment that satisfies the formula for \emph{one} value to universal variables then clearly the formula is not satisfiable for \emph{all} values.  Hence if \textsc{findCandidate} returns \texttt{NULL} there cannot exist a strategy for the bounded game.
\end{proof}

\begin{theorem}\label{theorem:solveAbstract}
    If there exists a strategy to a bounded game then \textsc{solveAbstract} will return a certificate tree labelled with a partial strategy.
\end{theorem}
\begin{proof}
    Consider an abstract game tree $T$ with all opponent actions enumerated, i.e. $T$ is a concrete game tree. The call to \textsc{findCandidate} on line~\ref{line:findcandidate} produces a formula exactly equivalent to a full expansion of the quantified formula above. If there is a winning strategy then it will be found by the SAT query and returned as a labelling of the certificate tree $T$.  The computed strategy is winning so the call to \textsc{verify} must return false and the algorithm terminates.

    If $T$ does not yet contain all opponent actions then \textsc{verify} may return a spoiling strategy that is then appended to the tree. The call to \textsc{verify} recursively calls \textsc{solveAbstract} for the opponent, so if a spoiling strategy is found it must be a valid opponent strategy against the candidate by Proposition~\ref{prop:findcandidate}. Any action appended to the tree must be one that is winning for the opponent against the candidate and so cannot already exist in the abstraction. Hence the refinement grows monotonically with each call to \textsc{verify}. Thus, either \textsc{verify} returns false and the algorithm terminates, or eventually the abstraction is refined into a concrete game tree and terminates as shown above.
\end{proof}

\section{Optimisations}

The bounded realisability algorithm has an worst case running time that is exponential in the number of opponent actions when the entire search tree must be explored before discovering a winning strategy. In this section I present some optimisations that aim to prune the search tree as well as discover winning strategies earlier in the search.

\subsection{Bad State Learning}
\label{sec:boundedLearning}

The most important optimisation that allows the algorithm to avoid much of the search space is to record states that are known to be losing for one player. On subsequent calls to the SAT solver we encode these states in the candidate strategy formula (see Algorithm~\ref{alg:treeFormulaLearning}). Thus the algorithm avoids choosing moves that lead to states that are already known to be losing.

Bad states are learned from failed attempts to find a candidate. Enabling the optimisation triggers a call to \textsc{learn} in line~\ref{line:boundedLearning} of \textsc{findCandidate}.  If the SAT solver cannot find a candidate strategy for a given abstract game tree that means that there is a fixed prefix in the game tree for which the current player can never win. The state reached by playing the moves in the prefix must then be a losing state with some caveats. If the state is at the node with height $k$ and losing for the environment then we know that the environment cannot force to the error set in $k$ rounds. We do not know if the environment can force to the error set in $> k$ rounds. Therefore we record losing states for the environment in an array of sets of states $B^e$ indexed by the height at which the set is losing. For the controller, a losing state is losing for any run of length $>= k$. Instead of keeping an array we maintain a single set of losing states $B^c$ and block the entire set in all rounds of the game. As a consequence the algorithm is no longer complete: we can no longer find strategies that visit states in a round $<k$ after we discover that they are losing states for $>=k$. In practical use we are uninterested in these controller strategies since they are not winning for the unbounded game.

Additional states can be learned by expanding a single state into a set of losing states by greedily testing each variable of the state for inclusion in a \emph{cube} of states. If the formula remains unsatisfiable for the generalised cube then the entire set of states must also be losing. This technique is well known in the literature and can be efficiently implemented using a SAT solver capable of solving under assumptions~\cite{Een03}. The entire learning procedure is shown in Algorithm~\ref{alg:boundedLearning}. \textsc{learn} takes a state $s$, and a tree formula $f$ that was found to be losing for the player $k$ at height $k$. First, literals are greedily removed from $s$ and the resulting generalised cube is added to $B^c$ or $B^e$. It is also possible to compute several cubes using this technique by removing literals in several different orders but the additional cost was found to outweigh the benefit in practice.

\begin{algorithm}
    \caption{Modified Tree Formulas with Bad State Avoidance}
    \label{alg:treeFormulaLearning}
    \begin{algorithmic}[1]
        \Function{treeFormula}{$k, T$}
        \If{$\Call{height}{k, T} = 0$}
        \State \Return{ $\lnot \Call{$B^c$}{\cS_{T}}$ }
        \Else
        \State \Return{$\lnot \Call{$B^c$}{\cS_{T}} \land$ \\
            $$\bigwedge_{\langle e, n \rangle \in \Call{succ}{T}}(\Call{$\delta$}{\cS_T, \cU_e, \cC_T, \cS_n} \land U_e = \Call{action}{e} \land \Call{treeFormula}{k, n})$$
        }
        \EndIf
        \EndFunction
        \algstore{tf1}
    \end{algorithmic}

    \begin{algorithmic}[1]
        \algrestore{tf1}
        \Function{\textoverline{treeFormula}}{$k, T$}
        \If{$\Call{height}{k, T} = 0$}
        \State \Return{\Call{E}{$\cS_{T}$}}
        \Else
        \State \Return{\Call{$B^e$[\Call{height}{k,t}]}{$\cS_{T}$} $\lor$ \\
        $$\bigvee_{\langle e, n \rangle \in \Call{succ}{T}}(\Call{$\delta$}{\cS_T, \cU_T, \cC_e, \cS_n} \land C_e = \Call{action}{e} \land \Call{\textoverline{treeFormula}}{k, n})$$ }
        \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Learn an expanded cube of losing states}
    \label{alg:boundedLearning}
    \begin{algorithmic}
        \Function{learn}{$p, s, k, T, f$}
            \State $ \hat{s} \gets s$
            \For{$s \in \cS$}
            \State $sol \gets \Call{SATWithAssumptions}{\hat{s} \setminus \{ s \}, f}$
                \If{$sol = \texttt{NULL}$}
                    \State $\hat{s} \gets \hat{s} \setminus \{ s \}$
                \EndIf
            \EndFor
            \If{$p = \texttt{cont}$}
                \State $B^c \gets B^c \lor \hat{s}$
            \Else
                \For{$i \in [0,\ldots,k]$}
                    \State $B^e[i] \gets B^e[i] \lor \hat{s}$
                \EndFor
            \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Strategy Shortening}

Learning new bad states means reducing the search space for the algorithm. It follows that it is better to learn states earlier in the algorithm's execution. One problem with relying on SAT calls that assume cooperation is that there is no urgency to the returned candidate strategies. Consider the running example: the environment can reach the error set by setting \texttt{request} to 2 during two rounds. However, in the empty abstract game tree of a bounded game of length 3 or longer, there is no reason for the SAT solver to make the first action one of the requesting rounds if it can assume the environment will never grant any resources. The first action is important because the candidate strategy is derived from that. The candidate is what the opponent has the chance to respond to, so if the candidate does not do anything useful the opponent's response has the freedom to be equally apathetic about reaching its goal. This leads to much of the search space being explored unnecessarily until we learn a losing state.

Encouraging the SAT solver to find \emph{shorter} candidate strategies is a successful heuristic for mitigating this issue. Whilst it does require more SAT calls per call to \textsc{findCandidate} it can be efficiently implemented using incremental SAT solving and during our benchmarking we found the cost to be worthwhile. A strategy is shorter if following the strategy leads to a known bad state for the opponent in fewer game rounds. For the environment this is clearly analogous to reaching the error set sooner. For the controller it is less clear, we use states that have been learned to be losing for the environment for a particular game height. The intuition is that these states are more likely to be \emph{safe}, i.e. belonging to the winning region of the controller.

\begin{algorithm}
    \caption{Strategy Shortening}
    \label{alg:strategyShortening}
    \begin{algorithmic}[1]
        \Function{shorten}{$p, s, k, T$}
            \State $\hat{T} \gets $ \Call{extend}{$T$}
            \State $f \gets $ \IfElse{$p = \texttt{cont}$}{\Call{treeFormula}{$k, \hat{T}$}}{\Call{\textoverline{treeFormula}}{$k, \hat{T}$}} \EndIfElse
            \State $\alpha \gets \top$
            \State $cand \gets \Call{SAT}{s(\cS_{\hat{T}}) \land f}$
            \For{$l \in \Call{leaves}{T}$}
                \State $n \gets $ \Call{root}{$l$}
                \While{\Call{height}{$k, n$} $\neq 0$}
                    \If{$p = \texttt{cont}$}
                        \State $\hat{\alpha} \gets $ \Call{$B^e$[\Call{height}{$k, n$}]}{$\cS_n$}
                    \Else
                    \State $\hat{\alpha} \gets $ \Call{$B^c$}{$\cS_n$}
                    \EndIf
                    \State $sol \gets $ \Call{SATWithAssumptions}{$\alpha \land \hat{\alpha}$, $s(\cS_{\hat{T}}) \land f$}
                    \If{$sol \neq \texttt{NULL}$}
                    \State $\alpha \gets \alpha \land \hat{\alpha}$
                        \State $cand \gets sol$
                        \Break
                    \EndIf
                        
                    \State $n \gets $ \Call{succ}{$n$}
                \EndWhile
            \EndFor
            \State \Return $cand$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:strategyShortening} gives the pseudocode for the optimisation. First a formula $f$ representing the abstract game is constructed in the same way as in \textsc{findCandidate}. An additional constraint on the formula $\alpha$ is initialised to true. Then for a leaf $l$ in the game tree the algorithm greedily attempts to construct a candidate strategy such that the highest possible predecessor of $l$ is a known winning state. If a candidate is found then the constraint is added to $\alpha$ and the algorithm continues on to find candidates with constraints added to other branches of the tree. The solution to the SAT query with the most restrictive constraint is returned to be used as a candidate strategy.

\subsection{Default Actions}

During the search for a candidate strategy the SAT solver selects actions for the opponent as though the players are cooperating. Sometimes the result is an action that will always fail for the opponent. In many specifications the environment is given the option to fail as a way of modelling errors. For example, in a network driver specification error transitions may be used to model failed connections. When such a transition exists it will often be selected by the SAT solver (especially when the strategy shortening optimisation is enabled). Constantly selecting a bad action for the opponent significantly affects the performance of the algorithm because no bad states can be learned and the solver must refine the game abstraction to avoid the bad action. Additionally, if a candidate strategy was found by relying on a bad action then it will usually need to be backtracked. 

To avoid problematic action selection the solver can instead use some heuristic to select the arbitrary action required in the SAT call in \textsc{findCandidate}. This does not affect the correctness of the algorithm. If no candidate can be found with the opponent playing an arbitrary action then clearly the selected action (or a different opponent action that is winning) would have eventually been refined into the abstract game if the opponent instead cooperated. A simple action selection heuristic has been observed to improve the performance of the solver during benchmarking. Before the main algorithm executes two SAT calls are made with formulas constructed from \textsc{treeFormula} and \textsc{\textoverline{treeFormula}} called on an empty abstract game tree. From the result a mapping of height to \emph{default action} is made for each player. During \textsc{findCandidate} calls the arbitrary opponent actions are taken from the corresponding map at the appropriate height.


\section{Discussion}

The design of the algorithm is motivated by the desire to solve bounded safety games whilst avoiding the potential state explosion of computing the winning set. The key insight is to shift the emphasis from finding a winning set to finding winning strategies. The shift is made possible by searching for runs in an abstraction of the game and using the results to refine the abstraction. The advantage of this approach is that even when the winning set is difficult to represent symbolically (via a BDD or similar) a winning strategy may still be found. The reverse is also true: if the winning strategy requires too much branching it will become intractable to construct it using this algorithm. The difference can be likened to breadth-first versus depth-first search: the controllable predecessor used to construct a BDD explores all branches before progressing to the next game round, and bounded realisability explores traces through the full height of the bounded game before constructing branches in the abstraction. As with search, the best performing algorithm depends on the particular problem instance to be solved.

\subsection{Comparison to QBF}

As previously stated the bounded realisability problem is a specialisation of QBF and the algorithm presented in this chapter is a domain specific QBF solver. In Chapter~\ref{ch:relatedwork} several approaches to solving general QBF problems were described. The state of the art in QBF is focused on several areas: dependency analysis, circuit analysis, and counterexample guided search. 

A QBF solver may use dependency analysis to determine a partial ordering on decision making in the search. The bounded realisability algorithm is based on partial expansions and not search but the way that partial expansions are made is effectively a decision. In the case of bounded realisability formulas analysis will most likely not reveal a dependency tree significantly different from the linear quantifier prefix itself. The formula is constructed by unrolling a transition relation that takes states and actions as input and outputs states for the next iteration. It may be possible that the relation contains independent subformulas and some actions and states can be decided in parallel but the formula also contains constraints on states variables in the form of error and learned states. Thus it is unlikely that any action from a subsequent game round may be decided before an action in the current game round. One useful aspect of the dependency tree is that state variables are decided entirely by a prefix of action variable assignments. This information is used in bounded realisability when the \textsc{outcome} of a labelled game tree is used to create a subgame. 

The algorithm takes advantage of a higher level of information about the original problem than an analysis of the circuit could provide. A generalised approach to learning in QBF detects cubes of satisfying assignments and conflict clauses so that they are not reconsidered in other branches of the search after backtracking. Every unsatisfiable formula produced by \textsc{findCandidate} is effectively discovering either a satisfying assignment (when the environment loses) or a conflict clauses (when the controller loses). The specialised solver projects this information onto state variables and transmits the learned states between rounds of the game. This is only possible with the knowledge that the formula is constructed from an iterated unrolling of the transition relation of a game and it would be difficult to reproduce this level of learning in a general QBF solver.

The ability to learn states is also the primary difference between the specialised solver and the counterexample guided QBF algorithm proposed in \cite{Janota12}. More recent QBF approaches use counterexamples to guide an abstraction of the problem via clause selection instead of trees. It would interesting to apply these techniques to bounded realisability although the extension to unbounded synthesis in Chapter~\ref{ch:unbounded} would not be applicable without an abstract game tree.

\subsection{Model checking}

The concept of verifying programs by searching for counterexamples of a certain length with a SAT solver was first introduced in a bounded approach to model checking~\cite{Biere99}. Replacing counterexample traces with trees is the natural extension of this approach to realisability given that the realisability problem is modelled as a game of opposing players.  The move from model checking to realisability brings additional complexity to the problem but the efficiency of SAT is still exploited to discover counterexamples quickly.

\subsection{Related synthesis techniques}

Bounded synthesis~\cite{Finkbeiner13} uses an SMT solver to search for a bounded \emph{implementation} for an LTL specification. Lazy synthesis~\cite{Finkbeiner12} similarly searches for a bounded partial implementation and uses BDD based model checking to search for counterexamples in order to refine the partial implementation. Both of these techniques are for full LTL synthesis, which is a different problem to the safety specifications solved in this chapter, but the overarching framework is similar. 

Algorithms that avoid BDDs in favour of SAT solving have been proposed for safety synthesis in the past~\cite{Morgenstern13,Chiang15,Bloem14} but none of these take the approach of unrolling the transition relation to a bound. In these previous works states belonging to a winning region for one player are collected over a series of SAT queries concerning single transitions. These works have more in common with the extension of bounded realisability to unbounded games in Chapter~\ref{ch:unbounded}.

\subsection{Limitations}

The performance of bounded realisability is influenced primarily on the branching factor of the game tree. The worst case scenario occurs when each environment action must be matched with a different controller action and no state learning is possible. For example, consider a modified version of the example given in Section~\ref{sec:boundedexample} in which the environment requests are not latched into a state variable.

\begin{itemize}
    \item $\cS = \{ \texttt{error} \} $. The only state is whether or not an error has occurred. 
    \item $\cU = \{ \texttt{os\_request} \} $. 
    \item $\cC = \{ \texttt{dev\_cmd} \}$.
    \item The transition relation $\delta$ is now: $$ \texttt{err'} \gets \texttt{os\_request} \neq \texttt{dev\_cmd} $$
    \item $s_0 = (\texttt{err} = 0)$. 
\end{itemize}

Without a state variable to learn the algorithm is forced to explore all possible actions to determine realisability. If the example is modified again to allow for more types of requests by increasing the domain of the action variables then it is easy to see the potential blow up. In Figure~\ref{fig:limitationexample} the final AGT for this example with a bound of $3$ and action variables of size $3$ is shown. The figure shows the values of \texttt{os\_request} in all possible paths through the game. It is clear that this algorithm cannot scale with these kinds of specifications. In this particular example learning that the state $\texttt{err} = 0$ is losing for the environment a various game rounds will significantly reduce the size of the tree. This example can be trivially extended so that learning will no longer be helpful by introducing states that record the history of the game. The extension can be done in a way that ensures that even with cube generalisation a unique state is learned in every node of the tree and learning does not reduce the search space.

It should be noted that it is trivial to solve this example with a BDD solver, which can immediately prove that $\texttt{err} = 0$ is a winning region for the controller. The extension to unbounded realisability in Chapter~\ref{ch:unbounded} will also be able to handle this example by constructing the winning region.

\begin{figure}
    \centering
    \begin{tikzpicture}[
        align at top,
        level distance = 10mm,
        level 1/.style={sibling distance=45mm},
        level 2/.style={sibling distance=15mm},
        level 3/.style={sibling distance=5mm}]
        \node [circle,draw] (root){}
            child {node [circle,draw] {}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [left=2mm] {\texttt{0}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right] {\texttt{1}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right=2mm] {\texttt{2}}
                }
                edge from parent node [above left] {\texttt{0}}
            }
            child {node [circle,draw] {}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [left=2mm] {\texttt{0}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right] {\texttt{1}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right=2mm] {\texttt{2}}
                }
                edge from parent node [right] {\texttt{1}}
            }
            child {node [circle,draw] {}
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [left=2mm] {\texttt{0}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right] {\texttt{1}}
                }
                child {node [circle,draw] {}
                    child {node [circle,draw] {}
                        edge from parent node [left] {\texttt{0}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right=-1.5mm] {\texttt{1}}
                    }
                    child {node [circle,draw] {}
                        edge from parent node [right] {\texttt{2}}
                    }
                    edge from parent node [right=2mm] {\texttt{2}}
                }
                edge from parent node [above right] {\texttt{2}}
            }
            node [left=4pt] {}
            node [above=4pt] {$\langle 0, 3 \rangle$};
    \end{tikzpicture}
    \caption{AGT with large branching factor}
    \label{fig:limitationexample}
\end{figure}

\subsection{Strengths}

Bounded realisability is most useful in the case where the winning region of a game has a large BDD but the winning strategy for the game is compact. It is not fair to compare the incomplete bounded algorithm to a complete winning region computation but we may consider the ability for each technique to find counterexamples. 

To demonstrate the usefulness of the algorithm we introduce a simple warehouse robot controller. In this example the warehouse consists of four loading bays and the robot is tasked with shipping items placed in the bays in a timely fashion. We model the problem with a \texttt{timer} variable that begins at one, ticks down to zero, and then resets back to one. At the beginning of the cycle the environment may place items in any two bays. The robot then may ship all items in one bay per timer tick and must clear all bays before the timer resets. The example is trivial but it could be scaled on the number of bays, number of items that the environment can load, and the length of the timer to produce complex specifications. In this example we use integer values for \texttt{timer}, \texttt{ship}, \texttt{load0} and \texttt{load1} to make the description more concise.

\begin{itemize}
    \item $\cS = \{ \texttt{error, bay0, bay1, bay2, bay3, timer} \} $.
    \item $\cU = \{ \texttt{load0, load1} \} $. 
    \item $\cC = \{ \texttt{ship} \}$.
    \item The transition relation $\delta$ is now: 
        \begin{align*}
            \texttt{error'} \gets {}& \texttt{timer} = 1 \land (\texttt{bay0} = 1 \lor \texttt{bay1} = 1 \lor \texttt{bay2} = 1 \lor \texttt{bay3} = 1 ) \\
            \texttt{bay0'} \gets {}& (\texttt{timer} = 1 \land (\texttt{load0} = 0 \lor \texttt{load1} = 0)) \\
            {}& \lor (\texttt{timer}\neq1 \land \texttt{ship} = 0) \\
            \texttt{bay1'} \gets {}& (\texttt{timer} = 1 \land (\texttt{load0} = 1 \lor \texttt{load1} = 1)) \\
            {}& \lor (\texttt{timer}\neq1 \land \texttt{ship} = 1) \\
            \texttt{bay2'} \gets {}& (\texttt{timer} = 1 \land (\texttt{load0} = 2 \lor \texttt{load1} = 2)) \\
            {}& \lor (\texttt{timer}\neq1 \land \texttt{ship} = 2) \\
            \texttt{bay3'} \gets {}& (\texttt{timer} = 1 \land (\texttt{load0} = 3 \lor \texttt{load1} = 3)) \\
            {}& \lor (\texttt{timer}\neq1 \land \texttt{ship} = 3) \\
            \texttt{timer'} \gets {}& (\texttt{timer} = 0)\ \texttt{?}\ 1\ \texttt{:}\ (\texttt{timer} - 1)
        \end{align*}
    \item $\begin{aligned}[t]
            s_0 = ({}& \texttt{error} = 0 \land \texttt{bay0} = 0 \land \texttt{bay1} = 0 \land \texttt{bay2} = 0 \\
            {}&\land \texttt{bay3} = 0 \land \texttt{timer} = 1)
        \end{aligned}$
\end{itemize}

The specification is clearly unrealisable given that in every cycle the environment can load items into two bays and the controller can only remove items from one bay. A realisability solver that uses BDDs might attempt to compute a winning region for the environment. The entire environment winning region for this game contains all possible configurations in which the environment loads two distinct bays. As shown in Figure~\ref{fig:strengthsexamplebdd}, BDDs are not succinct when used to represent formulas that are in this enumeration of cubes style. If this problem was to scale the BDD would quickly consume a large amount of space.

\tikzset{>={Latex[width=2.2mm,length=2.5mm]}}
\tikzstyle{one}=[->,solid]
\tikzstyle{zero}=[->,dash pattern = on 2pt off 2pt]
\begin{figure}
    \centering
    \captionsetup{justification=centering}
    \begin{tikzpicture}[level distance = 20mm,baseline]
        \node [circle,draw] (bay0){\texttt{bay0}}
            child {node [circle,draw] (bay1a) {\texttt{bay1}}
                child {node [circle,draw] (bay2a) {\texttt{bay2}}
                    child {node [circle,draw] (bay3a) {\texttt{bay3}}
                        child {node [draw] (T) {\texttt{T}}
                            edge from parent [zero]
                        }
                        edge from parent [zero]
                    }
                    edge from parent [one]
                }
                child {node [circle,draw] (bay2b) {\texttt{bay2}}
                    edge from parent [zero]
                }
                edge from parent [one]
            }
            child {node [circle,draw] (bay1b) {\texttt{bay1}}
                child {node [circle,draw,right] (bay2c) {\texttt{bay2}}
                    child {node [circle,draw] (bay3b) {\texttt{bay3}}
                        child {node [draw] (F) {\texttt{F}}
                            edge from parent [zero]
                        }
                        edge from parent [one]
                    }
                    edge from parent [zero]
                }
                edge from parent [zero]
            }
        ;
        \draw [one,->] (bay3a) to (F);
        \draw [one,->] (bay3b) to (T);
        \draw [one,->] (bay2b) to (bay3a);
        \draw [zero,->] (bay2b) to (bay3b);
        \draw [one,->] (bay1b) to (bay2b);
        \draw [one,->] (bay2a) to (F);
        \draw [zero,->,out=320,in=40] (bay2c) to (F);
    \end{tikzpicture}
    \caption[Environment winning region as a BDD]{Environment winning region as a BDD.\\Solid transitions are $1$, dashed transitions are $0$.}
    \label{fig:strengthsexamplebdd}
\end{figure}

By instead using a SAT solver to check for the existence of a spoiling strategy to the environment filling two bays, the bounded realisability algorithm avoids computing the set of all environment winning states. The difference between the two methodologies is similar to the difference between breadth first search (BFS) and depth first search (DFS). The BDD driven approach explores the game tree level by level similar to BFS and builds a compact set of winning states. The SAT based algorithm presented here explores entire paths through the tree, similar to DFS, by searching for traces. Both approaches are efficient on different classes of specifications.

\section{Summary}

In this chapter I presented the fundamental building block of this thesis, a new algorithm for solving bounded realisability. In later chapters I will explain extensions to this algorithm to increase its applicability and in Chapter~\ref{ch:evaluation} I will present results and an evaluation of the contribution of this work.

\begin{itemize}
    \item Here I introduce an algorithm for solving synthesis games that are bounded to a fixed number of game rounds. The algorithm is a counterexample guided abstraction refinement framework in which abstractions of the game are constructed from candidate strategies for the players. This is done in a way that allows a candidate strategy to be checked for a spoiling strategy by playing the game abstraction on behalf of the opponent. Spoiling strategies are counterexamples to a strategy that may be used for refinement.

    \item I presented several optimisations to the algorithm including computational learning of losing states and two heuristics for discovering more useful candidate strategies.

    \item The design of the algorithm is inspired by the exponential blow up that can result from constructing a symbolic representation of the winning region as a BDD. In this algorithm the winning region is never computed although some winning states are learned as an optimisation to prune the search tree. In Chapter~\ref{ch:unbounded} we will see how this algorithm may be extended to unbounded synthesis by approximating the winning region during the execution of the algorithm.


\end{itemize}
